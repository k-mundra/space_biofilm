{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Space Biofilm Project Main Data Reading File\n",
        "\n",
        "This file reads the ground and flight data to create a ground-> flight regression model based on the percent coverage of the biofilm. It predicts trajectories per base material to predict what the third day's percent coverage of the biofilm looks like. \n",
        "\n",
        "## Background \n",
        "\n",
        "The data in the OSD datasets are not spatiotemporally connected, which means they cannot be used to predict future morphology (shape) of biofilms from day to day. We did write a synthetic data code that serves as validation for morphology, so a future dataset could theoretically be tested using that code. Instead, the focus shifted to looking at the percentage of the sample coupon that was covered in a biofilm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNHIbuhpEkXw",
        "outputId": "3888faa5-7bcd-461a-a5ef-94abf0a52b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consolidated duplicate columns: [('Biofilm mass (µm^3/µm^2)', 2)]\n",
            "Done. Outputs in: /content/lsds55_outputs\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "This file does the following, and is the first file that was used to create a baseline method for this file.\n",
        "- Reads 'GROUND DATA' and 'FLIGHT DATA' from *_SUBMITTED.xlsx\n",
        "- Cleans headers; consolidates duplicate columns; coerces numerics\n",
        "- Parses Material AND robust BaseMaterial + DayInt\n",
        "- (A) Ground→Flight regression (dynamic features)\n",
        "- (B) Domain-aware regression with BaseMaterial + DayInt + condition\n",
        "- (C) Trajectories per BaseMaterial; predict Flight Day 3 (Methods A & B)\n",
        "- Writes CSVs, PNGs, JSON metrics to /content/lsds55_outputs\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "OUTDIR = Path(\"/content/lsds55_outputs\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "\n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        "\n",
        "    df = df.copy()\n",
        "    merged_info = []\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        merged_info.append((name, len(idxs)))\n",
        "    if merged_info:\n",
        "        print(\"Consolidated duplicate columns:\", merged_info)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "  \n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        return {\"note\": \"no rows for evaluation\"}\n",
        "    out = {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "    }\n",
        "    out[\"r2\"] = float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan\n",
        "    out[\"n\"] = int(len(y_true))\n",
        "    return out\n",
        "\n",
        "xl = pd.ExcelFile(EXCEL_PATH)\n",
        "have_ground = \"GROUND DATA\" in xl.sheet_names\n",
        "have_flight = \"FLIGHT DATA\" in xl.sheet_names\n",
        "if not (have_ground or have_flight):\n",
        "    raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' sheets found.\")\n",
        "\n",
        "dfs = []\n",
        "if have_ground:\n",
        "    g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "    g = normalize_headers(g).dropna(how=\"all\")\n",
        "    g[\"condition\"] = \"ground\"\n",
        "    dfs.append(g)\n",
        "\n",
        "if have_flight:\n",
        "    f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "    f = normalize_headers(f).dropna(how=\"all\")\n",
        "    f[\"condition\"] = \"flight\"\n",
        "    dfs.append(f)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = clean_cols(df_all)\n",
        "\n",
        "# Consolidate duplicates before coercion\n",
        "cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "\n",
        "# Parse Material & 'day' (legacy) + robust BaseMaterial & DayInt (for modeling)\n",
        "if \"Material and Incubation day\" not in df_all.columns:\n",
        "    raise RuntimeError(\"Missing 'Material and Incubation day' after header normalization/merge.\")\n",
        "df_all[[\"Material\", \"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "df_all[[\"BaseMaterial\", \"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "\n",
        "# Coerce numerics\n",
        "df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "\n",
        "# Keep essentials\n",
        "df = df_all.dropna(subset=[\"Material\", \"day\", \"BaseMaterial\", \"DayInt\", \"condition\"]).copy()\n",
        "df.to_csv(OUTDIR / \"cleaned_combined.csv\", index=False)\n",
        "\n",
        "existing_feats = [c for c in EXPECTED_FEATURES if c in df.columns]\n",
        "missing_feats = [c for c in EXPECTED_FEATURES if c not in df.columns]\n",
        "\n",
        "# Fallback: all numeric columns except TARGET & day-like fields\n",
        "if len(existing_feats) < len(EXPECTED_FEATURES):\n",
        "    numeric_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    fallback_exclude = {TARGET, \"day\", \"DayInt\", \"condition_bin\"}\n",
        "    fallback_feats = [c for c in numeric_candidates if c not in fallback_exclude]\n",
        "    feature_cols = existing_feats + [c for c in fallback_feats if c not in existing_feats]\n",
        "else:\n",
        "    feature_cols = existing_feats\n",
        "\n",
        "# Remove columns that are all-NaN\n",
        "feature_cols = [c for c in feature_cols if df[c].notna().any()]\n",
        "\n",
        "summary = {\n",
        "    \"rows_total\": int(len(df_all)),\n",
        "    \"rows_used\": int(len(df)),\n",
        "    \"conditions\": df[\"condition\"].value_counts(dropna=False).to_dict(),\n",
        "    \"materials_legacy\": df[\"Material\"].value_counts().to_dict(),\n",
        "    \"base_materials\": df[\"BaseMaterial\"].value_counts().to_dict(),\n",
        "    \"days_legacy\": df[\"day\"].value_counts().sort_index().to_dict(),\n",
        "    \"days\": df[\"DayInt\"].value_counts().sort_index().to_dict(),\n",
        "    \"features_used_for_A\": feature_cols,\n",
        "    \"features_missing_from_expected\": missing_feats,\n",
        "}\n",
        "with open(OUTDIR / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "metrics = {}\n",
        "if (\"ground\" in df[\"condition\"].unique()) and (\"flight\" in df[\"condition\"].unique()):\n",
        "    df_g = df[(df[\"condition\"]==\"ground\") & df[TARGET].notna()]\n",
        "    df_f = df[(df[\"condition\"]==\"flight\") & df[TARGET].notna()]\n",
        "    if not df_g.empty and not df_f.empty and len(feature_cols) > 0:\n",
        "        X_train = df_g[feature_cols].values\n",
        "        y_train = df_g[TARGET].values\n",
        "        X_test  = df_f[feature_cols].values\n",
        "        y_test  = df_f[TARGET].values\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred_f = rf.predict(X_test)\n",
        "\n",
        "        metrics[\"g2f_regression\"] = met(y_test, y_pred_f)\n",
        "        metrics[\"g2f_regression\"].update({\n",
        "            \"n_train_ground\": int(len(df_g)),\n",
        "            \"n_test_flight\": int(len(df_f)),\n",
        "            \"n_features\": int(len(feature_cols)),\n",
        "        })\n",
        "\n",
        "        pred_df = df_f.copy()\n",
        "        pred_df[\"predicted_coverage_g2f\"] = y_pred_f\n",
        "        pred_df.to_csv(OUTDIR / \"g2f_regression_predictions.csv\", index=False)\n",
        "\n",
        "        # Pred vs actual\n",
        "        plt.figure()\n",
        "        plt.scatter(y_test, y_pred_f)\n",
        "        plt.xlabel(\"Actual Flight Coverage\")\n",
        "        plt.ylabel(\"Predicted Flight Coverage (trained on Ground)\")\n",
        "        plt.title(\"Ground→Flight coverage regression\")\n",
        "        lo = float(min(y_test.min(), y_pred_f.min()))\n",
        "        hi = float(max(y_test.max(), y_pred_f.max()))\n",
        "        plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTDIR / \"g2f_pred_vs_actual.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Feature importances\n",
        "        imp = pd.DataFrame({\"feature\": feature_cols, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
        "        imp.to_csv(OUTDIR / \"g2f_feature_importances.csv\", index=False)\n",
        "        if len(feature_cols) > 1:\n",
        "            plt.figure()\n",
        "            plt.barh(imp[\"feature\"], imp[\"importance\"])\n",
        "            plt.xlabel(\"Importance\")\n",
        "            plt.title(\"RF Feature Importances (G→F)\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(OUTDIR / \"g2f_feature_importances.png\", dpi=150)\n",
        "            plt.close()\n",
        "    else:\n",
        "        metrics[\"g2f_regression\"] = {\"note\": \"Insufficient ground/flight rows with target or no usable features.\"}\n",
        "else:\n",
        "    metrics[\"g2f_regression\"] = {\"note\": \"Require both ground and flight; one missing.\"}\n",
        "\n",
        "df_b = df[df[TARGET].notna()].copy()\n",
        "df_b[\"condition_bin\"] = (df_b[\"condition\"]==\"flight\").astype(int)\n",
        "\n",
        "# Build numeric features dynamically for (B)\n",
        "num_candidates = df_b.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_candidates = [c for c in num_candidates if c != TARGET]\n",
        "# Always include DayInt and condition_bin if present; put them at the end for clarity\n",
        "num_cols = [c for c in num_candidates if c not in [\"DayInt\",\"condition_bin\"]] + [c for c in [\"DayInt\",\"condition_bin\"] if c in num_candidates or c in df_b.columns]\n",
        "num_cols = [c for c in num_cols if c in df_b.columns]\n",
        "cat_cols = [\"BaseMaterial\"] if \"BaseMaterial\" in df_b.columns else []\n",
        "\n",
        "if not df_b.empty and len(num_cols + cat_cols) > 0:\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), num_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    rf_dom = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "    pipe_dom = Pipeline([(\"pre\", pre), (\"rf\", rf_dom)])\n",
        "    pipe_dom.fit(df_b[num_cols + cat_cols], df_b[TARGET])\n",
        "\n",
        "    df_b[\"pred_domain_model_base\"] = pipe_dom.predict(df_b[num_cols + cat_cols])\n",
        "    df_b.to_csv(OUTDIR / \"domain_aware_predictions_base_material.csv\", index=False)\n",
        "\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        sub = df_b[df_b[\"condition\"]==cond]\n",
        "        if not sub.empty:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = met(sub[TARGET].values, sub[\"pred_domain_model_base\"].values)\n",
        "            metrics[f\"domain_model_base_{cond}\"][\"n\"] = int(len(sub))\n",
        "        else:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = {\"note\": \"no rows\"}\n",
        "else:\n",
        "    metrics[\"domain_model_base\"] = {\"note\": \"No rows or no usable features for domain-aware model.\"}\n",
        "\n",
        "plots_dir = OUTDIR / \"trajectories_base\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "traj_rows = []\n",
        "have_flight = \"flight\" in df[\"condition\"].unique()\n",
        "materials = sorted(df[\"BaseMaterial\"].dropna().unique())\n",
        "\n",
        "for mat in materials:\n",
        "    g_sub = df[(df[\"condition\"]==\"ground\") & (df[\"BaseMaterial\"]==mat)]\n",
        "    f_sub = df[(df[\"condition\"]==\"flight\") & (df[\"BaseMaterial\"]==mat)] if have_flight else pd.DataFrame()\n",
        "\n",
        "    # Need ≥2 ground days to fit slope\n",
        "    if g_sub[\"DayInt\"].nunique() < 2:\n",
        "        continue\n",
        "\n",
        "    # Linear ground trajectory: coverage ~ DayInt\n",
        "    Xg = g_sub[[\"DayInt\"]].values\n",
        "    yg = g_sub[TARGET].values\n",
        "    lin = LinearRegression().fit(Xg, yg)\n",
        "\n",
        "    slope = float(lin.coef_[0])\n",
        "    intercept = float(lin.intercept_)\n",
        "\n",
        "    # Ground per-day growth rate if Day1 and Day3 exist\n",
        "    growth_rate = np.nan\n",
        "    if (g_sub[\"DayInt\"]==1).any() and (g_sub[\"DayInt\"]==3).any():\n",
        "        cov_d1 = g_sub.loc[g_sub[\"DayInt\"]==1, TARGET].mean()\n",
        "        cov_d3 = g_sub.loc[g_sub[\"DayInt\"]==3, TARGET].mean()\n",
        "        growth_rate = (cov_d3 - cov_d1) / 2.0\n",
        "\n",
        "    # Method A: ground line at day 3\n",
        "    pred_day3_A = float(lin.predict(np.array([[3]]) )[0])\n",
        "\n",
        "    # Method B: Flight Day1 + ground growth_rate * 2 (if available)\n",
        "    pred_day3_B = np.nan\n",
        "    actual_flight_d3 = np.nan\n",
        "    if have_flight and not f_sub.empty:\n",
        "        f_d1 = f_sub[f_sub[\"DayInt\"]==1]\n",
        "        f_d3 = f_sub[f_sub[\"DayInt\"]==3]\n",
        "        if not f_d3.empty:\n",
        "            actual_flight_d3 = float(f_d3[TARGET].mean())\n",
        "        if not f_d1.empty and not np.isnan(growth_rate):\n",
        "            pred_day3_B = float(f_d1[TARGET].mean() + 2.0*growth_rate)\n",
        "\n",
        "    traj_rows.append({\n",
        "        \"BaseMaterial\": mat,\n",
        "        \"ground_lin_slope\": slope,\n",
        "        \"ground_lin_intercept\": intercept,\n",
        "        \"ground_growth_rate_per_day\": growth_rate,\n",
        "        \"predicted_flight_day3_methodA_ground_line\": pred_day3_A,\n",
        "        \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\": pred_day3_B,\n",
        "        \"actual_flight_day3\": actual_flight_d3\n",
        "    })\n",
        "\n",
        "    # Plot per base material\n",
        "    plt.figure()\n",
        "    plt.scatter(g_sub[\"DayInt\"], g_sub[TARGET], label=\"Ground (actual)\")\n",
        "    xs = np.array([[1],[2],[3]])\n",
        "    ys = lin.predict(xs)\n",
        "    plt.plot(xs.flatten(), ys, linestyle=\"--\", label=\"Ground fit\")\n",
        "    if have_flight and not f_sub.empty:\n",
        "        plt.scatter(f_sub[\"DayInt\"], f_sub[TARGET], label=\"Flight (actual)\")\n",
        "        if not np.isnan(pred_day3_B):\n",
        "            plt.scatter([3], [pred_day3_B], marker=\"x\", label=\"Pred Flight D3 (Method B)\")\n",
        "        plt.scatter([3], [pred_day3_A], marker=\"^\", label=\"Pred Flight D3 (Method A)\")\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Biofilm surface area coverage (%)\")\n",
        "    plt.title(f\"Trajectory (BaseMaterial): {mat}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plots_dir / f\"trajectory_{re.sub(r'[^A-Za-z0-9]+','_',mat)}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "traj_df = pd.DataFrame(traj_rows)\n",
        "traj_df.to_csv(OUTDIR / \"trajectory_predictions_base_material.csv\", index=False)\n",
        "\n",
        "# Evaluate trajectory predictions where Flight Day3 exists\n",
        "def eval_block(df_eval, pred_col):\n",
        "    if df_eval.empty or pred_col not in df_eval.columns:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    mask = (~df_eval[\"actual_flight_day3\"].isna()) & (~df_eval[pred_col].isna())\n",
        "    y_true = df_eval.loc[mask, \"actual_flight_day3\"].values\n",
        "    y_pred = df_eval.loc[mask, pred_col].values\n",
        "    if len(y_true) == 0:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\": float(r2_score(y_true, y_pred)) if len(np.unique(y_true))>1 else np.nan,\n",
        "        \"n\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "metrics[\"trajectory_base_methodA\"] = eval_block(traj_df, \"predicted_flight_day3_methodA_ground_line\")\n",
        "metrics[\"trajectory_base_methodB\"] = eval_block(traj_df, \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\")\n",
        "\n",
        "# Save metrics + README\n",
        "with open(OUTDIR / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "with open(OUTDIR / \"README.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "f\"\"\"Outputs: {OUTDIR}\n",
        "\"\"\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Xy6BzGuVy9"
      },
      "source": [
        "quadratic (degree-2) trajectory fit for the Ground curves, then using that to predict Flight Day-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIi6Goxkuegj",
        "outputId": "362d8453-3b88-4065-ab4a-c897fbe7304f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consolidated duplicate columns: [('Biofilm mass (µm^3/µm^2)', 2)]\n",
            "Done. Outputs in: /content/lsds55_outputs\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "numeric only pipeline with two sheet excel for LSDS\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy.polynomial.polynomial import polyfit, polyval\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "OUTDIR = Path(\"/content/lsds55_outputs\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  \n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    \"\"\"Return (Material, day_int) from strings like 'SS316 day1' or 'Silicone  day 3'.\"\"\"\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "  \n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        " \n",
        "    df = df.copy()\n",
        "    merged_info = []\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        merged_info.append((name, len(idxs)))\n",
        "    if merged_info:\n",
        "        print(\"Consolidated duplicate columns:\", merged_info)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Coerce listed columns to numeric, dropping extra duplicates if any remain.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        return {\"note\": \"no rows for evaluation\"}\n",
        "    out = {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "    }\n",
        "    out[\"r2\"] = float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan\n",
        "    out[\"n\"] = int(len(y_true))\n",
        "    return out\n",
        "\n",
        "xl = pd.ExcelFile(EXCEL_PATH)\n",
        "have_ground = \"GROUND DATA\" in xl.sheet_names\n",
        "have_flight = \"FLIGHT DATA\" in xl.sheet_names\n",
        "if not (have_ground or have_flight):\n",
        "    raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' sheets found.\")\n",
        "\n",
        "dfs = []\n",
        "if have_ground:\n",
        "    g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "    g = normalize_headers(g).dropna(how=\"all\")\n",
        "    g[\"condition\"] = \"ground\"\n",
        "    dfs.append(g)\n",
        "\n",
        "if have_flight:\n",
        "    f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "    f = normalize_headers(f).dropna(how=\"all\")\n",
        "    f[\"condition\"] = \"flight\"\n",
        "    dfs.append(f)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = clean_cols(df_all)\n",
        "\n",
        "# Consolidate duplicates before coercion\n",
        "cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "\n",
        "# Parse Material & 'day' (legacy) + robust BaseMaterial & DayInt (for modeling)\n",
        "if \"Material and Incubation day\" not in df_all.columns:\n",
        "    raise RuntimeError(\"Missing 'Material and Incubation day' after header normalization/merge.\")\n",
        "df_all[[\"Material\", \"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "df_all[[\"BaseMaterial\", \"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "\n",
        "# Coerce numerics\n",
        "df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "\n",
        "# Keep essentials\n",
        "df = df_all.dropna(subset=[\"Material\", \"day\", \"BaseMaterial\", \"DayInt\", \"condition\"]).copy()\n",
        "df.to_csv(OUTDIR / \"cleaned_combined.csv\", index=False)\n",
        "\n",
        "existing_feats = [c for c in EXPECTED_FEATURES if c in df.columns]\n",
        "missing_feats = [c for c in EXPECTED_FEATURES if c not in df.columns]\n",
        "\n",
        "# Fallback: all numeric columns except TARGET & day-like fields\n",
        "if len(existing_feats) < len(EXPECTED_FEATURES):\n",
        "    numeric_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    fallback_exclude = {TARGET, \"day\", \"DayInt\", \"condition_bin\"}\n",
        "    fallback_feats = [c for c in numeric_candidates if c not in fallback_exclude]\n",
        "    feature_cols = existing_feats + [c for c in fallback_feats if c not in existing_feats]\n",
        "else:\n",
        "    feature_cols = existing_feats\n",
        "\n",
        "# Remove columns that are all-NaN\n",
        "feature_cols = [c for c in feature_cols if df[c].notna().any()]\n",
        "\n",
        "summary = {\n",
        "    \"rows_total\": int(len(df_all)),\n",
        "    \"rows_used\": int(len(df)),\n",
        "    \"conditions\": df[\"condition\"].value_counts(dropna=False).to_dict(),\n",
        "    \"materials_legacy\": df[\"Material\"].value_counts().to_dict(),\n",
        "    \"base_materials\": df[\"BaseMaterial\"].value_counts().to_dict(),\n",
        "    \"days_legacy\": df[\"day\"].value_counts().sort_index().to_dict(),\n",
        "    \"days\": df[\"DayInt\"].value_counts().sort_index().to_dict(),\n",
        "    \"features_used_for_A\": feature_cols,\n",
        "    \"features_missing_from_expected\": missing_feats,\n",
        "}\n",
        "with open(OUTDIR / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "metrics = {}\n",
        "if (\"ground\" in df[\"condition\"].unique()) and (\"flight\" in df[\"condition\"].unique()):\n",
        "    df_g = df[(df[\"condition\"]==\"ground\") & df[TARGET].notna()]\n",
        "    df_f = df[(df[\"condition\"]==\"flight\") & df[TARGET].notna()]\n",
        "    if not df_g.empty and not df_f.empty and len(feature_cols) > 0:\n",
        "        X_train = df_g[feature_cols].values\n",
        "        y_train = df_g[TARGET].values\n",
        "        X_test  = df_f[feature_cols].values\n",
        "        y_test  = df_f[TARGET].values\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "        rf.fit(X_train, y_train)\n",
        "        y_pred_f = rf.predict(X_test)\n",
        "\n",
        "        metrics[\"g2f_regression\"] = met(y_test, y_pred_f)\n",
        "        metrics[\"g2f_regression\"].update({\n",
        "            \"n_train_ground\": int(len(df_g)),\n",
        "            \"n_test_flight\": int(len(df_f)),\n",
        "            \"n_features\": int(len(feature_cols)),\n",
        "        })\n",
        "\n",
        "        pred_df = df_f.copy()\n",
        "        pred_df[\"predicted_coverage_g2f\"] = y_pred_f\n",
        "        pred_df.to_csv(OUTDIR / \"g2f_regression_predictions.csv\", index=False)\n",
        "\n",
        "        # Pred vs actual\n",
        "        plt.figure()\n",
        "        plt.scatter(y_test, y_pred_f)\n",
        "        plt.xlabel(\"Actual Flight Coverage\")\n",
        "        plt.ylabel(\"Predicted Flight Coverage (trained on Ground)\")\n",
        "        plt.title(\"Ground→Flight coverage regression\")\n",
        "        lo = float(min(y_test.min(), y_pred_f.min()))\n",
        "        hi = float(max(y_test.max(), y_pred_f.max()))\n",
        "        plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTDIR / \"g2f_pred_vs_actual.png\", dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # Feature importances\n",
        "        imp = pd.DataFrame({\"feature\": feature_cols, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
        "        imp.to_csv(OUTDIR / \"g2f_feature_importances.csv\", index=False)\n",
        "        if len(feature_cols) > 1:\n",
        "            plt.figure()\n",
        "            plt.barh(imp[\"feature\"], imp[\"importance\"])\n",
        "            plt.xlabel(\"Importance\")\n",
        "            plt.title(\"RF Feature Importances (G→F)\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(OUTDIR / \"g2f_feature_importances.png\", dpi=150)\n",
        "            plt.close()\n",
        "    else:\n",
        "        metrics[\"g2f_regression\"] = {\"note\": \"Insufficient ground/flight rows with target or no usable features.\"}\n",
        "else:\n",
        "    metrics[\"g2f_regression\"] = {\"note\": \"Require both ground and flight; one missing.\"}\n",
        "\n",
        "df_b = df[df[TARGET].notna()].copy()\n",
        "df_b[\"condition_bin\"] = (df_b[\"condition\"]==\"flight\").astype(int)\n",
        "\n",
        "# Build numeric features dynamically for (B)\n",
        "num_candidates = df_b.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_candidates = [c for c in num_candidates if c != TARGET]\n",
        "# Always include DayInt and condition_bin if present; put them at the end for clarity\n",
        "num_cols = [c for c in num_candidates if c not in [\"DayInt\",\"condition_bin\"]] + [c for c in [\"DayInt\",\"condition_bin\"] if c in df_b.columns]\n",
        "cat_cols = [\"BaseMaterial\"] if \"BaseMaterial\" in df_b.columns else []\n",
        "\n",
        "if not df_b.empty and len(num_cols + cat_cols) > 0:\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), num_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    rf_dom = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "    pipe_dom = Pipeline([(\"pre\", pre), (\"rf\", rf_dom)])\n",
        "    pipe_dom.fit(df_b[num_cols + cat_cols], df_b[TARGET])\n",
        "\n",
        "    df_b[\"pred_domain_model_base\"] = pipe_dom.predict(df_b[num_cols + cat_cols])\n",
        "    df_b.to_csv(OUTDIR / \"domain_aware_predictions_base_material.csv\", index=False)\n",
        "\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        sub = df_b[df_b[\"condition\"]==cond]\n",
        "        if not sub.empty:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = met(sub[TARGET].values, sub[\"pred_domain_model_base\"].values)\n",
        "            metrics[f\"domain_model_base_{cond}\"][\"n\"] = int(len(sub))\n",
        "        else:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = {\"note\": \"no rows\"}\n",
        "else:\n",
        "    metrics[\"domain_model_base\"] = {\"note\": \"No rows or no usable features for domain-aware model.\"}\n",
        "\n",
        "plots_dir = OUTDIR / \"trajectories_base_quad\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "traj_rows = []\n",
        "have_flight = \"flight\" in df[\"condition\"].unique()\n",
        "materials = sorted(df[\"BaseMaterial\"].dropna().unique())\n",
        "\n",
        "for mat in materials:\n",
        "    g_sub = df[(df[\"condition\"]==\"ground\") & (df[\"BaseMaterial\"]==mat)]\n",
        "    f_sub = df[(df[\"condition\"]==\"flight\") & (df[\"BaseMaterial\"]==mat)] if have_flight else pd.DataFrame()\n",
        "\n",
        "    # Need ≥2 ground days to fit a curve; deg=2 is stable with 2–3 points\n",
        "    if g_sub[\"DayInt\"].nunique() < 2:\n",
        "        continue\n",
        "\n",
        "    xg = g_sub[\"DayInt\"].values.astype(float)\n",
        "    yg = g_sub[TARGET].values.astype(float)\n",
        "\n",
        "    # Quadratic fit: coverage = a0 + a1*x + a2*x^2\n",
        "    coefs = polyfit(xg, yg, deg=2)  # returns [a0, a1, a2]\n",
        "    pred_day3_A = float(polyval(3.0, coefs))\n",
        "\n",
        "    # Ground \"per-day\" growth rate analogous to linear delta (optional diagnostic)\n",
        "    growth_rate = np.nan\n",
        "    if (g_sub[\"DayInt\"]==1).any() and (g_sub[\"DayInt\"]==3).any():\n",
        "        cov_d1 = g_sub.loc[g_sub[\"DayInt\"]==1, TARGET].mean()\n",
        "        cov_d3 = g_sub.loc[g_sub[\"DayInt\"]==3, TARGET].mean()\n",
        "        growth_rate = (cov_d3 - cov_d1) / 2.0\n",
        "\n",
        "    pred_day3_B = np.nan\n",
        "    actual_flight_d3 = np.nan\n",
        "    if have_flight and not f_sub.empty:\n",
        "        f_d1 = f_sub[f_sub[\"DayInt\"]==1]\n",
        "        f_d3 = f_sub[f_sub[\"DayInt\"]==3]\n",
        "        if not f_d3.empty:\n",
        "            actual_flight_d3 = float(f_d3[TARGET].mean())\n",
        "        if not f_d1.empty and not np.isnan(growth_rate):\n",
        "            # Keep Method B identical in spirit to linear version for comparability\n",
        "            pred_day3_B = float(f_d1[TARGET].mean() + 2.0*growth_rate)\n",
        "\n",
        "    # Save row\n",
        "    traj_rows.append({\n",
        "        \"BaseMaterial\": mat,\n",
        "        \"quad_a0\": float(coefs[0]),\n",
        "        \"quad_a1\": float(coefs[1]),\n",
        "        \"quad_a2\": float(coefs[2]),\n",
        "        \"ground_growth_rate_per_day_linearized\": growth_rate,\n",
        "        \"predicted_flight_day3_methodA_ground_quadratic\": pred_day3_A,\n",
        "        \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\": pred_day3_B,\n",
        "        \"actual_flight_day3\": actual_flight_d3\n",
        "    })\n",
        "\n",
        "    # Plot per base material\n",
        "    xs = np.array([1.0, 2.0, 3.0])\n",
        "    ys_fit = polyval(xs, coefs)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(xg, yg, label=\"Ground (actual)\")\n",
        "    plt.plot(xs, ys_fit, linestyle=\"--\", label=\"Ground quadratic fit\")\n",
        "    if have_flight and not f_sub.empty:\n",
        "        plt.scatter(f_sub[\"DayInt\"], f_sub[TARGET], label=\"Flight (actual)\")\n",
        "        if not np.isnan(pred_day3_B):\n",
        "            plt.scatter([3], [pred_day3_B], marker=\"x\", label=\"Pred Flight D3 (Method B)\")\n",
        "        plt.scatter([3], [pred_day3_A], marker=\"^\", label=\"Pred Flight D3 (Method A, quad)\")\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Biofilm surface area coverage (%)\")\n",
        "    plt.title(f\"Trajectory (Quadratic) — {mat}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plots_dir / f\"trajectory_quad_{re.sub(r'[^A-Za-z0-9]+','_',mat)}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "traj_df = pd.DataFrame(traj_rows)\n",
        "traj_df.to_csv(OUTDIR / \"trajectory_predictions_base_material_quad.csv\", index=False)\n",
        "\n",
        "# Evaluate trajectory predictions where Flight Day3 exists\n",
        "def eval_block(df_eval, pred_col):\n",
        "    if df_eval.empty or pred_col not in df_eval.columns:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    mask = (~df_eval[\"actual_flight_day3\"].isna()) & (~df_eval[pred_col].isna())\n",
        "    y_true = df_eval.loc[mask, \"actual_flight_day3\"].values\n",
        "    y_pred = df_eval.loc[mask, pred_col].values\n",
        "    if len(y_true) == 0:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\": float(r2_score(y_true, y_pred)) if len(np.unique(y_true))>1 else np.nan,\n",
        "        \"n\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "# Add quadratic metric keys to avoid overwriting linear ones\n",
        "metrics[\"trajectory_base_quad_methodA\"] = eval_block(traj_df, \"predicted_flight_day3_methodA_ground_quadratic\")\n",
        "metrics[\"trajectory_base_quad_methodB\"] = eval_block(traj_df, \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\")\n",
        "\n",
        "# Save metrics + README\n",
        "with open(OUTDIR / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "with open(OUTDIR / \"README.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "f\"\"\"Outputs: {OUTDIR}\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "print(\"Done. Outputs in:\", OUTDIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9URTGlhu2DCK"
      },
      "source": [
        "testing improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i_Z4dBP2EeU"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "experiment harness\n",
        "\"\"\"\n",
        "\n",
        "import re, json, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import statsmodels.formula.api as smf\n",
        "    HAS_STATSMODELS = True\n",
        "except Exception:\n",
        "    HAS_STATSMODELS = False\n",
        "\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "ROOT_OUT = Path(\"/content/lsds55_outputs\")\n",
        "EXP_OUT = ROOT_OUT / \"experiments\"\n",
        "EXP_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        return {\"note\": \"no rows for evaluation\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\": float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"n\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "def load_clean_data():\n",
        "    xl = pd.ExcelFile(EXCEL_PATH)\n",
        "    dfs = []\n",
        "    if \"GROUND DATA\" in xl.sheet_names:\n",
        "        g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "        g = normalize_headers(g).dropna(how=\"all\")\n",
        "        g[\"condition\"] = \"ground\"\n",
        "        dfs.append(g)\n",
        "    if \"FLIGHT DATA\" in xl.sheet_names:\n",
        "        f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "        f = normalize_headers(f).dropna(how=\"all\")\n",
        "        f[\"condition\"] = \"flight\"\n",
        "        dfs.append(f)\n",
        "    if not dfs:\n",
        "        raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' sheets found.\")\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "    df_all = clean_cols(df_all)\n",
        "    cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "    df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "\n",
        "    if \"Material and Incubation day\" not in df_all.columns:\n",
        "        raise RuntimeError(\"Missing 'Material and Incubation day'.\")\n",
        "    df_all[[\"Material\", \"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "    df_all[[\"BaseMaterial\", \"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "    df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "\n",
        "    df = df_all.dropna(subset=[\"Material\", \"day\", \"BaseMaterial\", \"DayInt\", \"condition\"]).copy()\n",
        "    return df\n",
        "\n",
        "def get_dynamic_features_for_g2f(df, include_dayint=False, add_interactions=False):\n",
        "    existing = [c for c in EXPECTED_FEATURES if c in df.columns and df[c].notna().any()]\n",
        "    if len(existing) < len(EXPECTED_FEATURES):\n",
        "        numeric_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        exclude = {TARGET, \"day\", \"DayInt\", \"condition_bin\"}\n",
        "        fallback = [c for c in numeric_candidates if c not in exclude]\n",
        "        feats = existing + [c for c in fallback if c not in existing]\n",
        "    else:\n",
        "        feats = existing\n",
        "    if add_interactions:\n",
        "        if (\"Biofilm mass (µm^3/µm^2)\" in df.columns) and (\"roughness coefficient Ra*\" in df.columns):\n",
        "            df[\"mass_x_roughness\"] = df[\"Biofilm mass (µm^3/µm^2)\"] * df[\"roughness coefficient Ra*\"]\n",
        "            feats.append(\"mass_x_roughness\")\n",
        "        if (\"Biofilm biomass mean thickness (µm)\" in df.columns) and (\"Biofilm Maximum thickness (µm)\" in df.columns):\n",
        "            df[\"mean_x_max\"] = df[\"Biofilm biomass mean thickness (µm)\"] * df[\"Biofilm Maximum thickness (µm)\"]\n",
        "            feats.append(\"mean_x_max\")\n",
        "    if include_dayint and (\"DayInt\" in df.columns):\n",
        "        feats.append(\"DayInt\")\n",
        "    feats = [c for c in feats if c in df.columns and df[c].notna().any()]\n",
        "    return df, feats\n",
        "\n",
        "def run_g2f(df, model_ctor, feature_setup_kwargs):\n",
        "    df_g = df[(df[\"condition\"]==\"ground\") & df[TARGET].notna()].copy()\n",
        "    df_f = df[(df[\"condition\"]==\"flight\") & df[TARGET].notna()].copy()\n",
        "    if df_g.empty or df_f.empty:\n",
        "        return {\"g2f_regression\": {\"note\": \"Insufficient ground/flight rows\"}}\n",
        "    df_aug, feats = get_dynamic_features_for_g2f(df, **feature_setup_kwargs)\n",
        "    if not feats:\n",
        "        return {\"g2f_regression\": {\"note\": \"No usable features\"}}\n",
        "    X_train = df_g[feats].values\n",
        "    y_train = df_g[TARGET].values\n",
        "    X_test  = df_f[feats].values\n",
        "    y_test  = df_f[TARGET].values\n",
        "\n",
        "    model = model_ctor()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    out = met(y_test, y_pred)\n",
        "    out.update({\"n_train_ground\": int(len(df_g)), \"n_test_flight\": int(len(df_f)), \"n_features\": int(len(feats))})\n",
        "    return {\"g2f_regression\": out}\n",
        "\n",
        "def run_domain_aware(df, model_ctor):\n",
        "    df_b = df[df[TARGET].notna()].copy()\n",
        "    if df_b.empty:\n",
        "        return {\"domain_model_base_ground\": {\"note\":\"no rows\"}, \"domain_model_base_flight\": {\"note\":\"no rows\"}}\n",
        "    df_b[\"condition_bin\"] = (df_b[\"condition\"]==\"flight\").astype(int)\n",
        "    num_candidates = df_b.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    num_candidates = [c for c in num_candidates if c != TARGET]\n",
        "    num_cols = [c for c in num_candidates if c not in [\"DayInt\",\"condition_bin\"]]\n",
        "    for c in [\"DayInt\",\"condition_bin\"]:\n",
        "        if c in df_b.columns and c not in num_cols:\n",
        "            num_cols.append(c)\n",
        "    cat_cols = [\"BaseMaterial\"] if \"BaseMaterial\" in df_b.columns else []\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), num_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    model = model_ctor()\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
        "    pipe.fit(df_b[num_cols + cat_cols], df_b[TARGET])\n",
        "\n",
        "    df_b[\"pred_domain\"] = pipe.predict(df_b[num_cols + cat_cols])\n",
        "    metrics = {}\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        sub = df_b[df_b[\"condition\"]==cond]\n",
        "        if not sub.empty:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = met(sub[TARGET].values, sub[\"pred_domain\"].values)\n",
        "            metrics[f\"domain_model_base_{cond}\"][\"n\"] = int(len(sub))\n",
        "        else:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = {\"note\":\"no rows\"}\n",
        "    return metrics\n",
        "\n",
        "def eval_traj(df_traj_rows, colname):\n",
        "    if not df_traj_rows:\n",
        "        return {\"note\":\"no comparable rows\"}\n",
        "    df_eval = pd.DataFrame(df_traj_rows)\n",
        "    if colname not in df_eval.columns:\n",
        "        return {\"note\":\"no comparable rows\"}\n",
        "    mask = (~df_eval[\"actual_flight_day3\"].isna()) & (~df_eval[colname].isna())\n",
        "    y_true = df_eval.loc[mask, \"actual_flight_day3\"].values\n",
        "    y_pred = df_eval.loc[mask, colname].values\n",
        "    if len(y_true)==0:\n",
        "        return {\"note\":\"no comparable rows\"}\n",
        "    return met(y_true, y_pred)\n",
        "\n",
        "def traj_linear(df, mean_reps=False):\n",
        "    rows = []\n",
        "    for mat in sorted(df[\"BaseMaterial\"].dropna().unique()):\n",
        "        g = df[(df[\"condition\"]==\"ground\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        f = df[(df[\"condition\"]==\"flight\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        if mean_reps:\n",
        "            g = g.groupby(\"DayInt\", as_index=False)[TARGET].mean()\n",
        "            f = f.groupby(\"DayInt\", as_index=False)[TARGET].mean()\n",
        "        if g[\"DayInt\"].nunique()<2:\n",
        "            continue\n",
        "        Xg = g[[\"DayInt\"]].values\n",
        "        yg = g[TARGET].values\n",
        "        lin = LinearRegression().fit(Xg, yg)\n",
        "        pred_A = float(lin.predict(np.array([[3]]))[0])\n",
        "        growth_rate = np.nan\n",
        "        if (g[\"DayInt\"]==1).any() and (g[\"DayInt\"]==3).any():\n",
        "            cov_d1 = g.loc[g[\"DayInt\"]==1, TARGET].mean()\n",
        "            cov_d3 = g.loc[g[\"DayInt\"]==3, TARGET].mean()\n",
        "            growth_rate = (cov_d3 - cov_d1)/2.0\n",
        "        pred_B = np.nan\n",
        "        actual_f3 = np.nan\n",
        "        if not f.empty:\n",
        "            f_d1 = f[f[\"DayInt\"]==1]\n",
        "            f_d3 = f[f[\"DayInt\"]==3]\n",
        "            if not f_d3.empty:\n",
        "                actual_f3 = float(f_d3[TARGET].mean())\n",
        "            if not f_d1.empty and not np.isnan(growth_rate):\n",
        "                pred_B = float(f_d1[TARGET].mean() + 2.0*growth_rate)\n",
        "        rows.append({\"BaseMaterial\":mat,\"pred_A\":pred_A,\"pred_B\":pred_B,\"actual_flight_day3\":actual_f3})\n",
        "    return rows\n",
        "\n",
        "def traj_quadratic(df):\n",
        "    from numpy.polynomial.polynomial import polyfit, polyval\n",
        "    rows = []\n",
        "    for mat in sorted(df[\"BaseMaterial\"].dropna().unique()):\n",
        "        g = df[(df[\"condition\"]==\"ground\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        f = df[(df[\"condition\"]==\"flight\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        if g[\"DayInt\"].nunique()<2:\n",
        "            continue\n",
        "        xg = g[\"DayInt\"].values.astype(float)\n",
        "        yg = g[TARGET].values.astype(float)\n",
        "        coefs = polyfit(xg, yg, deg=2)\n",
        "        pred_A = float(polyval(3.0, coefs))\n",
        "        growth_rate = np.nan\n",
        "        if (g[\"DayInt\"]==1).any() and (g[\"DayInt\"]==3).any():\n",
        "            cov_d1 = g.loc[g[\"DayInt\"]==1, TARGET].mean()\n",
        "            cov_d3 = g.loc[g[\"DayInt\"]==3, TARGET].mean()\n",
        "            growth_rate = (cov_d3 - cov_d1)/2.0\n",
        "        pred_B = np.nan\n",
        "        actual_f3 = np.nan\n",
        "        if not f.empty:\n",
        "            f_d1 = f[f[\"DayInt\"]==1]\n",
        "            f_d3 = f[f[\"DayInt\"]==3]\n",
        "            if not f_d3.empty:\n",
        "                actual_f3 = float(f_d3[TARGET].mean())\n",
        "            if not f_d1.empty and not np.isnan(growth_rate):\n",
        "                pred_B = float(f_d1[TARGET].mean() + 2.0*growth_rate)\n",
        "        rows.append({\"BaseMaterial\":mat,\"pred_A\":pred_A,\"pred_B\":pred_B,\"actual_flight_day3\":actual_f3})\n",
        "    return rows\n",
        "\n",
        "def traj_exponential(df):\n",
        "    rows = []\n",
        "    for mat in sorted(df[\"BaseMaterial\"].dropna().unique()):\n",
        "        g = df[(df[\"condition\"]==\"ground\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        f = df[(df[\"condition\"]==\"flight\") & (df[\"BaseMaterial\"]==mat)]\n",
        "        if g[\"DayInt\"].nunique()<2:\n",
        "            continue\n",
        "        xg = g[\"DayInt\"].values.astype(float)\n",
        "        yg = g[TARGET].values.astype(float)\n",
        "        coefs = np.polyfit(xg, np.log1p(yg), 1)\n",
        "        pred_A = float(np.expm1(np.polyval(coefs, 3.0)))\n",
        "        growth_rate = np.nan\n",
        "        if (g[\"DayInt\"]==1).any() and (g[\"DayInt\"]==3).any():\n",
        "            cov_d1 = g.loc[g[\"DayInt\"]==1, TARGET].mean()\n",
        "            cov_d3 = g.loc[g[\"DayInt\"]==3, TARGET].mean()\n",
        "            growth_rate = (cov_d3 - cov_d1)/2.0\n",
        "        pred_B = np.nan\n",
        "        actual_f3 = np.nan\n",
        "        if not f.empty:\n",
        "            f_d1 = f[f[\"DayInt\"]==1]\n",
        "            f_d3 = f[f[\"DayInt\"]==3]\n",
        "            if not f_d3.empty:\n",
        "                actual_f3 = float(f_d3[TARGET].mean())\n",
        "            if not f_d1.empty and not np.isnan(growth_rate):\n",
        "                pred_B = float(f_d1[TARGET].mean() + 2.0*growth_rate)\n",
        "        rows.append({\"BaseMaterial\":mat,\"pred_A\":pred_A,\"pred_B\":pred_B,\"actual_flight_day3\":actual_f3})\n",
        "    return rows\n",
        "\n",
        "def baseline_models(df):\n",
        "    def rf_orig():\n",
        "        return RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "    base_g2f = run_g2f(df, rf_orig, dict(include_dayint=False, add_interactions=False))\n",
        "    base_dom = run_domain_aware(df, lambda: RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1))\n",
        "    lin_rows = traj_linear(df, mean_reps=False)\n",
        "    base_trajA = eval_traj(lin_rows, \"pred_A\")\n",
        "    base_trajB = eval_traj(lin_rows, \"pred_B\")\n",
        "    out = {}\n",
        "    out.update(base_g2f)\n",
        "    out.update(base_dom)\n",
        "    out[\"trajectory_base_methodA\"] = base_trajA\n",
        "    out[\"trajectory_base_methodB\"] = base_trajB\n",
        "    return out\n",
        "\n",
        "def exp_rf_tuned(df):\n",
        "    def rf_tuned():\n",
        "        return RandomForestRegressor(\n",
        "            n_estimators=1000, max_depth=10, min_samples_leaf=2, max_features=\"sqrt\",\n",
        "            random_state=42, n_jobs=-1\n",
        "        )\n",
        "    g2f = run_g2f(df, rf_tuned, dict(include_dayint=False, add_interactions=False))\n",
        "    dom = run_domain_aware(df, rf_tuned)\n",
        "    return {\"RF_tuned\": {**g2f, **dom}}\n",
        "\n",
        "def exp_gb(df):\n",
        "    def gb():\n",
        "        return GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=3, random_state=42)\n",
        "    g2f = run_g2f(df, gb, dict(include_dayint=False, add_interactions=False))\n",
        "    dom = run_domain_aware(df, gb)\n",
        "    return {\"GB\": {**g2f, **dom}}\n",
        "\n",
        "def exp_feat_interactions(df):\n",
        "    def rf_orig():\n",
        "        return RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "    g2f = run_g2f(df.copy(), rf_orig, dict(include_dayint=False, add_interactions=True))\n",
        "    dom = run_domain_aware(df.copy(), lambda: RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1))\n",
        "    return {\"feat_interactions\": {**g2f, **dom}}\n",
        "\n",
        "def exp_feat_add_dayint_g2f(df):\n",
        "    def rf_orig():\n",
        "        return RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "    g2f = run_g2f(df.copy(), rf_orig, dict(include_dayint=True, add_interactions=False))\n",
        "    dom = run_domain_aware(df.copy(), lambda: RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1))\n",
        "    return {\"feat_add_dayint_g2f\": {**g2f, **dom}}\n",
        "\n",
        "def exp_traj_quadratic(df):\n",
        "    rows = traj_quadratic(df.copy())\n",
        "    return {\"traj_quadratic\": {\n",
        "        \"trajectory_base_methodA\": eval_traj(rows, \"pred_A\"),\n",
        "        \"trajectory_base_methodB\": eval_traj(rows, \"pred_B\"),\n",
        "    }}\n",
        "\n",
        "def exp_traj_exponential(df):\n",
        "    rows = traj_exponential(df.copy())\n",
        "    return {\"traj_exponential\": {\n",
        "        \"trajectory_base_methodA\": eval_traj(rows, \"pred_A\"),\n",
        "        \"trajectory_base_methodB\": eval_traj(rows, \"pred_B\"),\n",
        "    }}\n",
        "\n",
        "def exp_traj_replicate_mean(df):\n",
        "    rows = traj_linear(df.copy(), mean_reps=True)\n",
        "    return {\"traj_replicate_mean\": {\n",
        "        \"trajectory_base_methodA\": eval_traj(rows, \"pred_A\"),\n",
        "        \"trajectory_base_methodB\": eval_traj(rows, \"pred_B\"),\n",
        "    }}\n",
        "\n",
        "def exp_xgboost(df):\n",
        "    if not HAS_XGB:\n",
        "        return {\"XGBoost\": {\"g2f_regression\": {\"note\":\"xgboost not installed\"},\n",
        "                            \"domain_model_base_ground\": {\"note\":\"xgboost not installed\"},\n",
        "                            \"domain_model_base_flight\": {\"note\":\"xgboost not installed\"}}}\n",
        "    def xgb_ctor():\n",
        "        return xgb.XGBRegressor(\n",
        "            n_estimators=800, learning_rate=0.05, max_depth=5,\n",
        "            subsample=0.9, colsample_bytree=0.8, random_state=42, n_jobs=-1,\n",
        "            reg_lambda=1.0\n",
        "        )\n",
        "    g2f = run_g2f(df, xgb_ctor, dict(include_dayint=False, add_interactions=False))\n",
        "    dom = run_domain_aware(df, xgb_ctor)\n",
        "    return {\"XGBoost\": {**g2f, **dom}}\n",
        "\n",
        "def exp_hist_gb(df):\n",
        "    def hgb():\n",
        "        return HistGradientBoostingRegressor(max_depth=8, learning_rate=0.05, max_iter=500, random_state=42)\n",
        "    g2f = run_g2f(df, hgb, dict(include_dayint=False, add_interactions=False))\n",
        "    dom = run_domain_aware(df, hgb)\n",
        "    return {\"HistGradientBoosting\": {**g2f, **dom}}\n",
        "\n",
        "def exp_cv_rf(df):\n",
        "    # CV on G2F and Domain-aware\n",
        "    def cv_rf_g2f():\n",
        "        df_g = df[(df[\"condition\"]==\"ground\") & df[TARGET].notna()].copy()\n",
        "        df_f = df[(df[\"condition\"]==\"flight\") & df[TARGET].notna()].copy()\n",
        "        if df_g.empty or df_f.empty:\n",
        "            return {\"g2f_regression\": {\"note\":\"Insufficient ground/flight rows\"}}\n",
        "\n",
        "        df_aug, feats = get_dynamic_features_for_g2f(df.copy(), include_dayint=False, add_interactions=False)\n",
        "        if not feats:\n",
        "            return {\"g2f_regression\": {\"note\":\"No usable features\"}}\n",
        "\n",
        "        Xg, yg = df_g[feats].values, df_g[TARGET].values\n",
        "        Xf, yf = df_f[feats].values, df_f[TARGET].values\n",
        "\n",
        "        base = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "        grid = GridSearchCV(\n",
        "            estimator=base,\n",
        "            param_grid={\"max_depth\":[5,10,None], \"min_samples_leaf\":[1,2,4], \"max_features\":[None,\"sqrt\"]},\n",
        "            scoring=\"r2\",\n",
        "            cv=KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        )\n",
        "        grid.fit(Xg, yg)\n",
        "        best = grid.best_estimator_\n",
        "        ypred = best.predict(Xf)\n",
        "        out = met(yf, ypred)\n",
        "        out.update({\"cv_best_params\": grid.best_params_})\n",
        "        return {\"g2f_regression\": out}\n",
        "\n",
        "    def cv_rf_domain():\n",
        "        df_b = df[df[TARGET].notna()].copy()\n",
        "        if df_b.empty:\n",
        "            return {\"domain_model_base_ground\":{\"note\":\"no rows\"},\"domain_model_base_flight\":{\"note\":\"no rows\"}}\n",
        "        df_b[\"condition_bin\"] = (df_b[\"condition\"]==\"flight\").astype(int)\n",
        "        num_candidates = df_b.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        num_candidates = [c for c in num_candidates if c != TARGET]\n",
        "        num_cols = [c for c in num_candidates if c not in [\"DayInt\",\"condition_bin\"]]\n",
        "        for c in [\"DayInt\",\"condition_bin\"]:\n",
        "            if c in df_b.columns and c not in num_cols:\n",
        "                num_cols.append(c)\n",
        "        cat_cols = [\"BaseMaterial\"] if \"BaseMaterial\" in df_b.columns else []\n",
        "\n",
        "        pre = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\", StandardScaler(), num_cols),\n",
        "                (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
        "            ],\n",
        "            remainder=\"drop\"\n",
        "        )\n",
        "        base = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
        "        pipe = Pipeline([(\"pre\", pre), (\"model\", base)])\n",
        "        grid = GridSearchCV(\n",
        "            estimator=pipe,\n",
        "            param_grid={\n",
        "                \"model__max_depth\":[5,10,None],\n",
        "                \"model__min_samples_leaf\":[1,2,4],\n",
        "                \"model__max_features\":[None,\"sqrt\"]\n",
        "            },\n",
        "            scoring=\"r2\",\n",
        "            cv=KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        )\n",
        "        grid.fit(df_b[num_cols + cat_cols], df_b[TARGET])\n",
        "        df_b[\"pred_cv\"] = grid.predict(df_b[num_cols + cat_cols])\n",
        "        metrics = {}\n",
        "        for cond in [\"ground\",\"flight\"]:\n",
        "            sub = df_b[df_b[\"condition\"]==cond]\n",
        "            if not sub.empty:\n",
        "                m = met(sub[TARGET].values, sub[\"pred_cv\"].values)\n",
        "                m[\"cv_best_params\"] = grid.best_params_\n",
        "                m[\"n\"] = int(len(sub))\n",
        "                metrics[f\"domain_model_base_{cond}\"] = m\n",
        "            else:\n",
        "                metrics[f\"domain_model_base_{cond}\"] = {\"note\":\"no rows\"}\n",
        "        return metrics\n",
        "\n",
        "    out = {}\n",
        "    out.update(cv_rf_g2f())\n",
        "    out.update(cv_rf_domain())\n",
        "    return {\"CV_RandomForest\": out}\n",
        "\n",
        "def exp_combined_global(df):\n",
        "\n",
        "    metrics = {}\n",
        "    if not HAS_STATSMODELS:\n",
        "        return {\"Combined_Global\": {\n",
        "            \"domain_model_base_ground\":{\"note\":\"statsmodels not installed\"},\n",
        "            \"domain_model_base_flight\":{\"note\":\"statsmodels not installed\"},\n",
        "            \"trajectory_combined_methodA\":{\"note\":\"statsmodels not installed\"}\n",
        "        }}\n",
        "    df_b = df[df[TARGET].notna()].copy()\n",
        "    if df_b.empty:\n",
        "        return {\"Combined_Global\": {\n",
        "            \"domain_model_base_ground\":{\"note\":\"no rows\"},\n",
        "            \"domain_model_base_flight\":{\"note\":\"no rows\"},\n",
        "            \"trajectory_combined_methodA\":{\"note\":\"no rows\"}\n",
        "        }}\n",
        "\n",
        "    df_b[\"condition_bin\"] = (df_b[\"condition\"]==\"flight\").astype(int)\n",
        "\n",
        "    # Patsy formula with Q() escaping for the target\n",
        "    formula = f'Q(\"{TARGET}\") ~ DayInt * condition_bin + C(BaseMaterial)'\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        model = smf.ols(formula, data=df_b).fit()\n",
        "\n",
        "    df_b[\"pred_ols\"] = model.predict(df_b)\n",
        "\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        sub = df_b[df_b[\"condition\"]==cond]\n",
        "        if not sub.empty:\n",
        "            y_true = sub[TARGET].values\n",
        "            y_pred = sub[\"pred_ols\"].values\n",
        "            metrics[f\"domain_model_base_{cond}\"] = met(y_true, y_pred)\n",
        "            metrics[f\"domain_model_base_{cond}\"][\"n\"] = int(len(sub))\n",
        "        else:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = {\"note\":\"no rows\"}\n",
        "\n",
        "    # Predict Flight Day3 per BaseMaterial and evaluate vs actual\n",
        "    rows = []\n",
        "    for mat in sorted(df_b[\"BaseMaterial\"].dropna().unique()):\n",
        "        df_pred = pd.DataFrame({\"DayInt\":[3], \"condition_bin\":[1], \"BaseMaterial\":[mat]})\n",
        "        yhat = float(model.predict(df_pred)[0])\n",
        "        f_d3 = df_b[(df_b[\"BaseMaterial\"]==mat) & (df_b[\"condition\"]==\"flight\") & (df_b[\"DayInt\"]==3)]\n",
        "        actual = float(f_d3[TARGET].mean()) if not f_d3.empty else np.nan\n",
        "        rows.append({\"BaseMaterial\": mat, \"pred_ols_day3_flight\": yhat, \"actual_flight_day3\": actual})\n",
        "\n",
        "    df_eval = pd.DataFrame(rows)\n",
        "    mask = (~df_eval[\"actual_flight_day3\"].isna()) & (~df_eval[\"pred_ols_day3_flight\"].isna())\n",
        "    if mask.any():\n",
        "        metrics[\"trajectory_combined_methodA\"] = met(\n",
        "            df_eval.loc[mask,\"actual_flight_day3\"].values,\n",
        "            df_eval.loc[mask,\"pred_ols_day3_flight\"].values\n",
        "        )\n",
        "        metrics[\"trajectory_combined_methodA\"][\"n\"] = int(mask.sum())\n",
        "    else:\n",
        "        metrics[\"trajectory_combined_methodA\"] = {\"note\":\"no comparable rows\"}\n",
        "\n",
        "    return {\"Combined_Global\": metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99bpLcL76D-b",
        "outputId": "b0453b67-0a3c-4dfc-f5dc-2c5a6123f0d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Wrote:\n",
            "- /content/lsds55_outputs/experiments/metrics_comparison.json\n",
            "- /content/lsds55_outputs/experiments/metrics_comparison.csv\n",
            "- /content/lsds55_outputs/experiments/experiments_log.txt\n",
            "\n",
            "=== Quick winners per metric ===\n",
            "g2f_regression: best R2 → {'experiment': 'CV_RandomForest', 'r2': 0.9687976252136771}, lowest RMSE → {'experiment': 'CV_RandomForest', 'rmse': 6.951043703668633}\n",
            "domain_model_base_ground: best R2 → {'experiment': 'XGBoost', 'r2': 0.9999999992018583}, lowest RMSE → {'experiment': 'XGBoost', 'rmse': 0.000921669140147205}\n",
            "domain_model_base_flight: best R2 → {'experiment': 'XGBoost', 'r2': 0.9999999996688176}, lowest RMSE → {'experiment': 'XGBoost', 'rmse': 0.000716126590920895}\n",
            "trajectory_base_methodA: best R2 → {'experiment': 'baseline_recomputed', 'r2': -0.2585982876309145}, lowest RMSE → {'experiment': 'baseline_recomputed', 'rmse': 43.47417728653114}\n",
            "trajectory_base_methodB: best R2 → {'experiment': 'baseline_recomputed', 'r2': 0.03753894267906133}, lowest RMSE → {'experiment': 'baseline_recomputed', 'rmse': 38.01713517174416}\n",
            "trajectory_combined_methodA: best R2 → {'experiment': 'Combined_Global', 'r2': 0.8631917011591599}, lowest RMSE → {'experiment': 'Combined_Global', 'rmse': 14.333228522980269}\n"
          ]
        }
      ],
      "source": [
        "def safe_run(name, fn, *args, **kwargs):\n",
        "    \"\"\"Run an experiment function safely; on error, return a note.\"\"\"\n",
        "    try:\n",
        "        out = fn(*args, **kwargs)\n",
        "        if not isinstance(out, dict) or len(out) == 0:\n",
        "            return {name: {\"_runner_error\": {\"note\": \"empty or invalid experiment output\"}}}\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        return {name: {\"_runner_error\": {\"note\": f\"exception: {type(e).__name__}: {e}\"}}}\n",
        "\n",
        "df = load_clean_data()\n",
        "baseline = baseline_models(df)\n",
        "\n",
        "saved_baseline = {}\n",
        "metrics_json_path = ROOT_OUT / \"metrics.json\"\n",
        "if metrics_json_path.exists():\n",
        "    try:\n",
        "        saved_baseline = json.load(open(metrics_json_path, \"r\"))\n",
        "    except Exception:\n",
        "        saved_baseline = {}\n",
        "\n",
        "# Collect experiments with robust wrapper\n",
        "experiments = {}\n",
        "# prior\n",
        "experiments.update(safe_run(\"RF_tuned\", exp_rf_tuned, df))\n",
        "experiments.update(safe_run(\"GB\", exp_gb, df))\n",
        "experiments.update(safe_run(\"feat_interactions\", exp_feat_interactions, df))\n",
        "experiments.update(safe_run(\"feat_add_dayint_g2f\", exp_feat_add_dayint_g2f, df))\n",
        "experiments.update(safe_run(\"traj_quadratic\", exp_traj_quadratic, df))\n",
        "experiments.update(safe_run(\"traj_exponential\", exp_traj_exponential, df))\n",
        "experiments.update(safe_run(\"traj_replicate_mean\", exp_traj_replicate_mean, df))\n",
        "# new\n",
        "experiments.update(safe_run(\"XGBoost\", exp_xgboost, df))\n",
        "experiments.update(safe_run(\"HistGradientBoosting\", exp_hist_gb, df))\n",
        "experiments.update(safe_run(\"CV_RandomForest\", exp_cv_rf, df))\n",
        "experiments.update(safe_run(\"Combined_Global\", exp_combined_global, df))\n",
        "\n",
        "def metrics_to_rows(experiment_label, metrics_block):\n",
        "    \"\"\"\n",
        "    experiment_label: the experiment name to put in the 'experiment' column\n",
        "    metrics_block: dict like {\"g2f_regression\": {...}, \"domain_model_base_ground\": {...}, ...}\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for metric_name, val in metrics_block.items():\n",
        "        if isinstance(val, dict) and (\"rmse\" in val or \"note\" in val):\n",
        "            row = val.copy()\n",
        "            row[\"experiment\"] = experiment_label\n",
        "            row[\"metric\"] = metric_name\n",
        "            rows.append(row)\n",
        "    return rows\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Baselines\n",
        "rows += metrics_to_rows(\"baseline_recomputed\", baseline)\n",
        "if saved_baseline:\n",
        "    rows += metrics_to_rows(\"baseline_saved\", saved_baseline)\n",
        "\n",
        "# Experiments (use OUTER KEY as the experiment label)\n",
        "captured_experiments = []\n",
        "for exp_key, metrics_block in experiments.items():\n",
        "    captured_experiments.append(exp_key)\n",
        "    rows += metrics_to_rows(exp_key, metrics_block)\n",
        "\n",
        "cmp_df = pd.DataFrame(rows)\n",
        "\n",
        "# Sort for readability\n",
        "order = [\n",
        "    \"g2f_regression\",\n",
        "    \"domain_model_base_ground\", \"domain_model_base_flight\",\n",
        "    \"trajectory_base_methodA\", \"trajectory_base_methodB\",\n",
        "    \"trajectory_combined_methodA\"\n",
        "]\n",
        "if \"metric\" in cmp_df.columns:\n",
        "    cmp_df[\"metric\"] = pd.Categorical(cmp_df[\"metric\"], categories=order, ordered=True)\n",
        "    cmp_df = cmp_df.sort_values([\"metric\",\"experiment\"]).reset_index(drop=True)\n",
        "\n",
        "# Save comparison outputs\n",
        "cmp_json = EXP_OUT / \"metrics_comparison.json\"\n",
        "cmp_csv = EXP_OUT / \"metrics_comparison.csv\"\n",
        "cmp_df.to_json(cmp_json, orient=\"records\", indent=2)\n",
        "cmp_df.to_csv(cmp_csv, index=False)\n",
        "\n",
        "# Log which experiments were captured\n",
        "with open(EXP_OUT / \"experiments_log.txt\", \"w\") as fh:\n",
        "    fh.write(\"Captured experiments (experiment column labels):\\n\")\n",
        "    for lab in captured_experiments:\n",
        "        fh.write(f\"- {lab}\\n\")\n",
        "\n",
        "print(\"Done. Wrote:\")\n",
        "print(\"-\", cmp_json)\n",
        "print(\"-\", cmp_csv)\n",
        "print(\"-\", EXP_OUT / \"experiments_log.txt\")\n",
        "\n",
        "def best_by(metric_name, stat_key, better=\"max\"):\n",
        "    sub = cmp_df[(cmp_df[\"metric\"]==metric_name) & (~cmp_df[stat_key].isna())]\n",
        "    if sub.empty:\n",
        "        return {\"experiment\": None, stat_key: None}\n",
        "    idx = sub[stat_key].idxmax() if better==\"max\" else sub[stat_key].idxmin()\n",
        "    return {\"experiment\": sub.loc[idx, \"experiment\"], stat_key: float(sub.loc[idx, stat_key])}\n",
        "\n",
        "print(\"\\n=== winners per metric ===\")\n",
        "for m in order:\n",
        "    if m not in cmp_df[\"metric\"].unique():\n",
        "        print(f\"{m}: (no rows)\")\n",
        "        continue\n",
        "    w_r2 = best_by(m, \"r2\", better=\"max\")\n",
        "    w_rmse = best_by(m, \"rmse\", better=\"min\")\n",
        "    print(f\"{m}: best R2 → {w_r2}, lowest RMSE → {w_rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYBklYm8-d95",
        "outputId": "a353c053-4036-41ca-bac2-214bb06a6ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoE done. Wrote:\n",
            "- /content/lsds55_outputs/moe/moe_predictions.csv\n",
            "- /content/lsds55_outputs/moe/moe_metrics.json\n",
            "- /content/lsds55_outputs/moe/gate_report.json\n",
            "{\n",
            "  \"ExpertA_CVRandomForest_all\": {\n",
            "    \"rmse\": 1.8698,\n",
            "    \"mae\": 1.1574,\n",
            "    \"r2\": 0.9975,\n",
            "    \"n\": 36\n",
            "  },\n",
            "  \"ExpertB_LinearTrajectory_all\": {\n",
            "    \"rmse\": 17.0,\n",
            "    \"mae\": 13.9968,\n",
            "    \"r2\": 0.7953,\n",
            "    \"n\": 36\n",
            "  },\n",
            "  \"Mixture_all\": {\n",
            "    \"rmse\": 3.4911,\n",
            "    \"mae\": 2.8556,\n",
            "    \"r2\": 0.9914,\n",
            "    \"n\": 36\n",
            "  },\n",
            "  \"ExpertA_CVRandomForest_ground\": {\n",
            "    \"rmse\": 2.1731,\n",
            "    \"mae\": 1.423,\n",
            "    \"r2\": 0.9956,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"ExpertB_LinearTrajectory_ground\": {\n",
            "    \"rmse\": 18.7451,\n",
            "    \"mae\": 15.5832,\n",
            "    \"r2\": 0.6699,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"Mixture_ground\": {\n",
            "    \"rmse\": 3.0375,\n",
            "    \"mae\": 2.4501,\n",
            "    \"r2\": 0.9913,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"ExpertA_CVRandomForest_flight\": {\n",
            "    \"rmse\": 1.5067,\n",
            "    \"mae\": 0.8919,\n",
            "    \"r2\": 0.9985,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"ExpertB_LinearTrajectory_flight\": {\n",
            "    \"rmse\": 15.0541,\n",
            "    \"mae\": 12.4105,\n",
            "    \"r2\": 0.8536,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"Mixture_flight\": {\n",
            "    \"rmse\": 3.8921,\n",
            "    \"mae\": 3.2612,\n",
            "    \"r2\": 0.9902,\n",
            "    \"n\": 18\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Hybrid Mixture-of-Experts (MoE) for numeric only data\n",
        "\"\"\"\n",
        "\n",
        "import re, json, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "OUTDIR = Path(\"/content/lsds55_outputs/moe\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true)==0 or len(y_pred)==0:\n",
        "        return {\"note\":\"no rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\":  float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\":   float(r2_score(y_true, y_pred)) if len(np.unique(y_true))>1 else np.nan,\n",
        "        \"n\":    int(len(y_true))\n",
        "    }\n",
        "\n",
        "xl = pd.ExcelFile(EXCEL_PATH)\n",
        "dfs = []\n",
        "if \"GROUND DATA\" in xl.sheet_names:\n",
        "    g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "    g = normalize_headers(g).dropna(how=\"all\")\n",
        "    g[\"condition\"] = \"ground\"\n",
        "    dfs.append(g)\n",
        "if \"FLIGHT DATA\" in xl.sheet_names:\n",
        "    f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "    f = normalize_headers(f).dropna(how=\"all\")\n",
        "    f[\"condition\"] = \"flight\"\n",
        "    dfs.append(f)\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' found.\")\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = clean_cols(df_all)\n",
        "\n",
        "cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "\n",
        "if \"Material and Incubation day\" not in df_all.columns:\n",
        "    raise RuntimeError(\"Missing 'Material and Incubation day' after normalization.\")\n",
        "\n",
        "df_all[[\"Material\",\"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "df_all[[\"BaseMaterial\",\"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "\n",
        "df = df_all.dropna(subset=[\"BaseMaterial\",\"DayInt\",\"condition\", TARGET]).copy()\n",
        "df[\"condition_bin\"] = (df[\"condition\"]==\"flight\").astype(int)\n",
        "\n",
        "# Numeric features for Expert A (domain-aware)\n",
        "num_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "# Drop target & pure identifier-like columns\n",
        "numA = [c for c in num_candidates if c not in {TARGET}]\n",
        "# Keep DayInt & condition_bin; others are whatever numeric exist (mass/thickness/roughness)\n",
        "catA = [\"BaseMaterial\"]\n",
        "\n",
        "# Expert B features (trajectory-aware linear): DayInt, DayInt*condition, BaseMaterial\n",
        "df[\"DayInt_x_condition\"] = df[\"DayInt\"] * df[\"condition_bin\"]\n",
        "numB = [\"DayInt\", \"DayInt_x_condition\"]\n",
        "catB = [\"BaseMaterial\"]\n",
        "\n",
        "# For the gate we’ll use a slim, robust set\n",
        "gate_num = []\n",
        "for c in [\"DayInt\", \"condition_bin\"]:\n",
        "    if c in df.columns:\n",
        "        gate_num.append(c)\n",
        "# add a couple of stable numeric morphology features if present\n",
        "for c in [\"Biofilm mass (µm^3/µm^2)\",\"roughness coefficient Ra*\",\"Biofilm Maximum thickness (µm)\",\"Biofilm biomass mean thickness (µm)\"]:\n",
        "    if c in df.columns:\n",
        "        gate_num.append(c)\n",
        "gate_cat = [\"BaseMaterial\"]\n",
        "\n",
        "preA = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(with_mean=False), numA),  # with_mean=False keeps sparse-safe\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catA)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "rf_base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "rf_grid = {\n",
        "    \"model__max_depth\": [5, 10, None],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4],\n",
        "    \"model__max_features\": [None, \"sqrt\"]\n",
        "}\n",
        "pipeA = Pipeline([(\"pre\", preA), (\"model\", rf_base)])\n",
        "cvA = GridSearchCV(\n",
        "    pipeA, rf_grid, cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "    scoring=\"r2\", n_jobs=-1\n",
        ")\n",
        "\n",
        "preB = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(with_mean=False), numB),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catB)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "pipeB = Pipeline([(\"pre\", preB), (\"model\", LinearRegression())])\n",
        "\n",
        "# Build OOF predictions for both experts for gating labels\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "oof_idx = []\n",
        "oof_pred_A = np.zeros(len(df))\n",
        "oof_pred_B = np.zeros(len(df))\n",
        "\n",
        "X_A = df[numA + catA].copy()\n",
        "X_B = df[numB + catB].copy()\n",
        "y = df[TARGET].values\n",
        "\n",
        "for train_idx, val_idx in kfold.split(df):\n",
        "    # Expert A\n",
        "    cvA.fit(X_A.iloc[train_idx], y[train_idx])\n",
        "    predA = cvA.predict(X_A.iloc[val_idx])\n",
        "    oof_pred_A[val_idx] = predA\n",
        "    # Expert B\n",
        "    pipeB.fit(X_B.iloc[train_idx], y[train_idx])\n",
        "    predB = pipeB.predict(X_B.iloc[val_idx])\n",
        "    oof_pred_B[val_idx] = predB\n",
        "    oof_idx.extend(val_idx.tolist())\n",
        "\n",
        "# OOF rows may be out of order; ensure alignment\n",
        "oof_mask = np.zeros(len(df), dtype=bool)\n",
        "oof_mask[oof_idx] = True\n",
        "\n",
        "# Gating labels: 1 if Expert B is better (lower abs error), else 0\n",
        "abs_err_A = np.abs(y - oof_pred_A)\n",
        "abs_err_B = np.abs(y - oof_pred_B)\n",
        "gate_labels = (abs_err_B < abs_err_A).astype(int)\n",
        "\n",
        "cvA.fit(X_A, y)  # Expert A finalized\n",
        "pipeB.fit(X_B, y)  # Expert B finalized\n",
        "\n",
        "preGate = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(with_mean=False), gate_num),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), gate_cat)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "gate = Pipeline([\n",
        "    (\"pre\", preGate),\n",
        "    (\"clf\", LogisticRegression(max_iter=400, solver=\"lbfgs\"))\n",
        "])\n",
        "X_gate = df[gate_num + gate_cat].copy()\n",
        "gate.fit(X_gate, gate_labels)\n",
        "\n",
        "predA_all = cvA.predict(X_A)\n",
        "predB_all = pipeB.predict(X_B)\n",
        "wB = gate.predict_proba(X_gate)[:,1]  # weight for Expert B\n",
        "pred_mix = (1.0 - wB) * predA_all + wB * predB_all\n",
        "\n",
        "def add_metrics(prefix, y_true, y_hat, ddict):\n",
        "    ddict[prefix] = met(y_true, y_hat)\n",
        "\n",
        "metrics = {}\n",
        "add_metrics(\"ExpertA_CVRandomForest_all\", y, predA_all, metrics)\n",
        "add_metrics(\"ExpertB_LinearTrajectory_all\", y, predB_all, metrics)\n",
        "add_metrics(\"Mixture_all\", y, pred_mix, metrics)\n",
        "\n",
        "# Per condition\n",
        "for cond in [\"ground\",\"flight\"]:\n",
        "    idx = (df[\"condition\"]==cond).values\n",
        "    add_metrics(f\"ExpertA_CVRandomForest_{cond}\", y[idx], predA_all[idx], metrics)\n",
        "    add_metrics(f\"ExpertB_LinearTrajectory_{cond}\", y[idx], predB_all[idx], metrics)\n",
        "    add_metrics(f\"Mixture_{cond}\", y[idx], pred_mix[idx], metrics)\n",
        "\n",
        "pred_out = df[[\"Material and Incubation day\",\"BaseMaterial\",\"DayInt\",\"condition\",TARGET]].copy()\n",
        "pred_out[\"pred_expertA\"] = predA_all\n",
        "pred_out[\"pred_expertB\"] = predB_all\n",
        "pred_out[\"gate_wB\"] = wB\n",
        "pred_out[\"pred_mixture\"] = pred_mix\n",
        "pred_out.to_csv(OUTDIR / \"moe_predictions.csv\", index=False)\n",
        "\n",
        "json.dump(metrics, open(OUTDIR / \"moe_metrics.json\",\"w\"), indent=2)\n",
        "\n",
        "# Gate report (top coefficients if linear-ish)\n",
        "gate_report = {\"note\": \"gate is LogisticRegression over preprocessed features\"}\n",
        "try:\n",
        "    # Recover feature names from ColumnTransformer\n",
        "    ohe = gate.named_steps[\"pre\"].transformers_[1][1]  # (\"cat\", OneHotEncoder, gate_cat)\n",
        "    num_names = gate_num\n",
        "    cat_names = []\n",
        "    if hasattr(ohe, \"get_feature_names_out\"):\n",
        "        cat_names = list(ohe.get_feature_names_out(gate_cat))\n",
        "    feat_names = list(num_names) + list(cat_names)\n",
        "    coef = gate.named_steps[\"clf\"].coef_.ravel()\n",
        "    top_idx = np.argsort(np.abs(coef))[::-1][:15]\n",
        "    gate_report = {\n",
        "        \"top_features\": [\n",
        "            {\"feature\": feat_names[i] if i < len(feat_names) else f\"feat_{i}\",\n",
        "             \"coef\": float(coef[i])}\n",
        "            for i in top_idx\n",
        "        ],\n",
        "        \"intercept\": float(gate.named_steps[\"clf\"].intercept_[0])\n",
        "    }\n",
        "except Exception as e:\n",
        "    gate_report[\"warn\"] = f\"feature inspection failed: {e}\"\n",
        "\n",
        "json.dump(gate_report, open(OUTDIR / \"gate_report.json\",\"w\"), indent=2)\n",
        "\n",
        "print(\"MoE done. Wrote:\")\n",
        "print(\"-\", OUTDIR / \"moe_predictions.csv\")\n",
        "print(\"-\", OUTDIR / \"moe_metrics.json\")\n",
        "print(\"-\", OUTDIR / \"gate_report.json\")\n",
        "\n",
        "# (optional) quick console summary\n",
        "def pretty(m):\n",
        "    return {k: {kk: round(vv,4) if isinstance(vv, float) else vv for kk,vv in d.items()} for k,d in m.items()}\n",
        "print(json.dumps(pretty(metrics), indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbnc85bB-e3b",
        "outputId": "212ac8b4-5371-46ad-c2e8-02836cda6113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved per-material plots to: /content/lsds55_outputs/moe/trajectories_expertA\n",
            "Also saved combined grid: /content/lsds55_outputs/moe/trajectories_expertA/expertA_trajectories_all_materials.png\n"
          ]
        }
      ],
      "source": [
        "# Trajectory plots random forest for expert A\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ROOT = Path(\"/content/lsds55_outputs/moe\")\n",
        "PRED_CSV = ROOT / \"moe_predictions.csv\"\n",
        "OUTDIR = ROOT / \"trajectories_expertA\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(PRED_CSV)\n",
        "# Basic hygiene\n",
        "df[\"DayInt\"] = pd.to_numeric(df[\"DayInt\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"BaseMaterial\",\"DayInt\",\"condition\",\"Biofilm surface area coverage (%)\",\"pred_expertA\"])\n",
        "df[\"condition\"] = df[\"condition\"].str.lower().str.strip()\n",
        "\n",
        "# Helper: safe filename from material name\n",
        "def safe_name(s):\n",
        "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", str(s)).strip(\"_\")\n",
        "\n",
        "materials = sorted(df[\"BaseMaterial\"].unique())\n",
        "\n",
        "for mat in materials:\n",
        "    sub = df[df[\"BaseMaterial\"] == mat].copy()\n",
        "\n",
        "    plt.figure(figsize=(6,4.2))\n",
        "    # Actual points (scatter) and their per-day means (dashed line)\n",
        "    for cond, marker in [(\"ground\",\"o\"), (\"flight\",\"s\")]:\n",
        "        csub = sub[sub[\"condition\"]==cond]\n",
        "        if csub.empty:\n",
        "            continue\n",
        "        # scatter all replicates\n",
        "        plt.scatter(csub[\"DayInt\"], csub[\"Biofilm surface area coverage (%)\"],\n",
        "                    label=f\"{cond.capitalize()} actual\", marker=marker, alpha=0.8)\n",
        "        # day-wise mean of actuals\n",
        "        m_actual = csub.groupby(\"DayInt\")[\"Biofilm surface area coverage (%)\"].mean().sort_index()\n",
        "        if len(m_actual) > 1:\n",
        "            plt.plot(m_actual.index.values, m_actual.values, linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "    # Expert A predictions: per-day mean line per condition\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        csub = sub[sub[\"condition\"]==cond]\n",
        "        if csub.empty:\n",
        "            continue\n",
        "        m_pred = csub.groupby(\"DayInt\")[\"pred_expertA\"].mean().sort_index()\n",
        "        plt.plot(m_pred.index.values, m_pred.values, linewidth=2, label=f\"{cond.capitalize()} ExpertA (mean)\")\n",
        "\n",
        "    plt.title(f\"Expert A Trajectory — {mat}\")\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Biofilm surface area coverage (%)\")\n",
        "    plt.xticks(sorted(sub[\"DayInt\"].unique()))\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTDIR / f\"expertA_trajectory_{safe_name(mat)}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "print(f\"Saved per-material plots to: {OUTDIR}\")\n",
        "\n",
        "n = len(materials)\n",
        "cols = 3\n",
        "rows = int(np.ceil(n / cols))\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols*5.0, rows*3.6), squeeze=False)\n",
        "\n",
        "for idx, mat in enumerate(materials):\n",
        "    r, c = divmod(idx, cols)\n",
        "    ax = axes[r, c]\n",
        "    sub = df[df[\"BaseMaterial\"]==mat]\n",
        "    # Actual points\n",
        "    for cond, marker in [(\"ground\",\"o\"), (\"flight\",\"s\")]:\n",
        "        csub = sub[sub[\"condition\"]==cond]\n",
        "        if csub.empty:\n",
        "            continue\n",
        "        ax.scatter(csub[\"DayInt\"], csub[\"Biofilm surface area coverage (%)\"], marker=marker, alpha=0.8, label=f\"{cond[:1].upper()} act\")\n",
        "        m_actual = csub.groupby(\"DayInt\")[\"Biofilm surface area coverage (%)\"].mean().sort_index()\n",
        "        if len(m_actual)>1:\n",
        "            ax.plot(m_actual.index.values, m_actual.values, linestyle=\"--\", alpha=0.6)\n",
        "    # Expert A mean preds\n",
        "    for cond in [\"ground\",\"flight\"]:\n",
        "        csub = sub[sub[\"condition\"]==cond]\n",
        "        if csub.empty:\n",
        "            continue\n",
        "        m_pred = csub.groupby(\"DayInt\")[\"pred_expertA\"].mean().sort_index()\n",
        "        ax.plot(m_pred.index.values, m_pred.values, linewidth=2, label=f\"{cond[:1].upper()} A\")\n",
        "\n",
        "    ax.set_title(mat)\n",
        "    ax.set_xlabel(\"Day\")\n",
        "    ax.set_ylabel(\"Coverage (%)\")\n",
        "    ax.set_xticks(sorted(sub[\"DayInt\"].unique()))\n",
        "    ax.grid(False)\n",
        "\n",
        "# tidy legends: only on last axis\n",
        "for r in range(rows):\n",
        "    for c in range(cols):\n",
        "        if r*cols + c >= n:\n",
        "            axes[r, c].axis(\"off\")\n",
        "# Put a single legend\n",
        "handles, labels = axes[0,0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"upper center\", ncol=6, frameon=False)\n",
        "fig.tight_layout(rect=[0,0,1,0.95])\n",
        "combined_path = OUTDIR / \"expertA_trajectories_all_materials.png\"\n",
        "fig.savefig(combined_path, dpi=150)\n",
        "plt.close(fig)\n",
        "\n",
        "print(f\"Also saved combined grid: {combined_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M7B0uuKA0ml",
        "outputId": "13fc0805-597b-4c22-ddab-27e4aedb2e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blind-test metrics (rounded):\n",
            "{\n",
            "  \"randomTrain_ExpertA\": {\n",
            "    \"rmse\": 1.801,\n",
            "    \"mae\": 1.247,\n",
            "    \"r2\": 0.9978,\n",
            "    \"n\": 25\n",
            "  },\n",
            "  \"randomTrain_ExpertB\": {\n",
            "    \"rmse\": 13.7902,\n",
            "    \"mae\": 11.4291,\n",
            "    \"r2\": 0.8689,\n",
            "    \"n\": 25\n",
            "  },\n",
            "  \"randomTrain_Mixture\": {\n",
            "    \"rmse\": 4.492,\n",
            "    \"mae\": 3.3786,\n",
            "    \"r2\": 0.9861,\n",
            "    \"n\": 25\n",
            "  },\n",
            "  \"randomTest_ExpertA\": {\n",
            "    \"rmse\": 5.3952,\n",
            "    \"mae\": 3.4357,\n",
            "    \"r2\": 0.978,\n",
            "    \"n\": 11\n",
            "  },\n",
            "  \"randomTest_ExpertB\": {\n",
            "    \"rmse\": 23.7824,\n",
            "    \"mae\": 20.4639,\n",
            "    \"r2\": 0.5722,\n",
            "    \"n\": 11\n",
            "  },\n",
            "  \"randomTest_Mixture\": {\n",
            "    \"rmse\": 9.0319,\n",
            "    \"mae\": 7.7858,\n",
            "    \"r2\": 0.9383,\n",
            "    \"n\": 11\n",
            "  },\n",
            "  \"G2F_train_ground_ExpertA\": {\n",
            "    \"rmse\": 3.5242,\n",
            "    \"mae\": 2.3066,\n",
            "    \"r2\": 0.9883,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"G2F_train_ground_ExpertB\": {\n",
            "    \"rmse\": 16.2495,\n",
            "    \"mae\": 12.7293,\n",
            "    \"r2\": 0.7519,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"G2F_train_ground_Mixture\": {\n",
            "    \"rmse\": 4.6252,\n",
            "    \"mae\": 3.6367,\n",
            "    \"r2\": 0.9799,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"G2F_test_flight_ExpertA\": {\n",
            "    \"rmse\": 8.2159,\n",
            "    \"mae\": 6.5053,\n",
            "    \"r2\": 0.9564,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"G2F_test_flight_ExpertB\": {\n",
            "    \"rmse\": 31.8104,\n",
            "    \"mae\": 25.2709,\n",
            "    \"r2\": 0.3465,\n",
            "    \"n\": 18\n",
            "  },\n",
            "  \"G2F_test_flight_Mixture\": {\n",
            "    \"rmse\": 16.6606,\n",
            "    \"mae\": 12.198,\n",
            "    \"r2\": 0.8207,\n",
            "    \"n\": 18\n",
            "  }\n",
            "}\n",
            "\n",
            "Wrote:\n",
            "- /content/lsds55_outputs/moe_blind/moe_blind_metrics.json\n",
            "- /content/lsds55_outputs/moe_blind/moe_blind_predictions_random_split.csv\n",
            "- /content/lsds55_outputs/moe_blind/moe_blind_predictions_ground_to_flight.csv\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "blind testing moe model \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import re, json, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "OUTDIR = Path(\"/content/lsds55_outputs/moe_blind\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true)==0 or len(y_pred)==0:\n",
        "        return {\"note\":\"no rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\":  float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\":   float(r2_score(y_true, y_pred)) if len(np.unique(y_true))>1 else np.nan,\n",
        "        \"n\":    int(len(y_true))\n",
        "    }\n",
        "\n",
        "def load_clean_df():\n",
        "    xl = pd.ExcelFile(EXCEL_PATH)\n",
        "    dfs = []\n",
        "    if \"GROUND DATA\" in xl.sheet_names:\n",
        "        g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "        g = normalize_headers(g).dropna(how=\"all\")\n",
        "        g[\"condition\"] = \"ground\"\n",
        "        dfs.append(g)\n",
        "    if \"FLIGHT DATA\" in xl.sheet_names:\n",
        "        f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "        f = normalize_headers(f).dropna(how=\"all\")\n",
        "        f[\"condition\"] = \"flight\"\n",
        "        dfs.append(f)\n",
        "    if not dfs:\n",
        "        raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' found.\")\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "    df_all = clean_cols(df_all)\n",
        "    cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "    df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "    if \"Material and Incubation day\" not in df_all.columns:\n",
        "        raise RuntimeError(\"Missing 'Material and Incubation day' after normalization.\")\n",
        "    df_all[[\"Material\",\"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "    df_all[[\"BaseMaterial\",\"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "    df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "    df = df_all.dropna(subset=[\"BaseMaterial\",\"DayInt\",\"condition\", TARGET]).copy()\n",
        "    df[\"condition_bin\"] = (df[\"condition\"]==\"flight\").astype(int)\n",
        "    df[\"DayInt\"] = pd.to_numeric(df[\"DayInt\"], errors=\"coerce\")\n",
        "    df[\"DayInt_x_condition\"] = df[\"DayInt\"] * df[\"condition_bin\"]\n",
        "    return df\n",
        "\n",
        "def fit_moe(train_df):\n",
        "    \"\"\"Fit MoE on train_df, return dict with trained pipelines and feature lists.\"\"\"\n",
        "    df = train_df.copy()\n",
        "\n",
        "    # Expert A feature sets\n",
        "    num_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numA = [c for c in num_candidates if c != TARGET]\n",
        "    catA = [\"BaseMaterial\"]\n",
        "\n",
        "    # Expert B (trajectory)\n",
        "    numB = [\"DayInt\", \"DayInt_x_condition\"]\n",
        "    numB = [c for c in numB if c in df.columns]\n",
        "    catB = [\"BaseMaterial\"]\n",
        "\n",
        "    # Gate features: small, robust set\n",
        "    gate_num = []\n",
        "    for c in [\"DayInt\", \"condition_bin\"]:\n",
        "        if c in df.columns:\n",
        "            gate_num.append(c)\n",
        "    for c in [\"Biofilm mass (µm^3/µm^2)\", \"roughness coefficient Ra*\", \"Biofilm Maximum thickness (µm)\", \"Biofilm biomass mean thickness (µm)\"]:\n",
        "        if c in df.columns:\n",
        "            gate_num.append(c)\n",
        "    gate_cat = [\"BaseMaterial\"]\n",
        "\n",
        "    y = df[TARGET].values\n",
        "\n",
        "    # Expert A: CV-tuned RF\n",
        "    preA = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(with_mean=False), numA),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catA)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    rf_base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "    rf_grid = {\n",
        "        \"model__max_depth\": [5, 10, None],\n",
        "        \"model__min_samples_leaf\": [1, 2, 4],\n",
        "        \"model__max_features\": [None, \"sqrt\"]\n",
        "    }\n",
        "    pipeA = Pipeline([(\"pre\", preA), (\"model\", rf_base)])\n",
        "    cvA = GridSearchCV(\n",
        "        pipeA, rf_grid,\n",
        "        cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        scoring=\"r2\", n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Expert B: linear trajectory\n",
        "    preB = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(with_mean=False), numB),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catB)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    pipeB = Pipeline([(\"pre\", preB), (\"model\", LinearRegression())])\n",
        "\n",
        "    # OOF preds for gate labels\n",
        "    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    oof_pred_A = np.zeros(len(df))\n",
        "    oof_pred_B = np.zeros(len(df))\n",
        "    X_A = df[numA + catA].copy()\n",
        "    X_B = df[numB + catB].copy()\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(df):\n",
        "        cvA.fit(X_A.iloc[train_idx], y[train_idx])\n",
        "        oof_pred_A[val_idx] = cvA.predict(X_A.iloc[val_idx])\n",
        "        pipeB.fit(X_B.iloc[train_idx], y[train_idx])\n",
        "        oof_pred_B[val_idx] = pipeB.predict(X_B.iloc[val_idx])\n",
        "\n",
        "    abs_err_A = np.abs(y - oof_pred_A)\n",
        "    abs_err_B = np.abs(y - oof_pred_B)\n",
        "    gate_labels = (abs_err_B < abs_err_A).astype(int)\n",
        "\n",
        "    # Fit experts on all training data\n",
        "    cvA.fit(X_A, y)\n",
        "    pipeB.fit(X_B, y)\n",
        "\n",
        "    # Fit gate on training data\n",
        "    preGate = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(with_mean=False), gate_num),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), gate_cat)\n",
        "        ],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    gate = Pipeline([\n",
        "        (\"pre\", preGate),\n",
        "        (\"clf\", LogisticRegression(max_iter=400, solver=\"lbfgs\"))\n",
        "    ])\n",
        "    X_gate = df[gate_num + gate_cat].copy()\n",
        "    gate.fit(X_gate, gate_labels)\n",
        "\n",
        "    return {\n",
        "        \"cvA\": cvA,\n",
        "        \"pipeB\": pipeB,\n",
        "        \"gate\": gate,\n",
        "        \"numA\": numA,\n",
        "        \"catA\": catA,\n",
        "        \"numB\": numB,\n",
        "        \"catB\": catB,\n",
        "        \"gate_num\": gate_num,\n",
        "        \"gate_cat\": gate_cat,\n",
        "    }\n",
        "\n",
        "def eval_moe(model, df, split_name):\n",
        "    \"\"\"Return metrics dict and prediction DataFrame for a given split.\"\"\"\n",
        "    d = df.copy()\n",
        "    y = d[TARGET].values\n",
        "\n",
        "    X_A = d[model[\"numA\"] + model[\"catA\"]].copy()\n",
        "    X_B = d[model[\"numB\"] + model[\"catB\"]].copy()\n",
        "    X_gate = d[model[\"gate_num\"] + model[\"gate_cat\"]].copy()\n",
        "\n",
        "    predA = model[\"cvA\"].predict(X_A)\n",
        "    predB = model[\"pipeB\"].predict(X_B)\n",
        "    wB = model[\"gate\"].predict_proba(X_gate)[:,1]\n",
        "    predMix = (1.0 - wB) * predA + wB * predB\n",
        "\n",
        "    metrics = {\n",
        "        f\"{split_name}_ExpertA\": met(y, predA),\n",
        "        f\"{split_name}_ExpertB\": met(y, predB),\n",
        "        f\"{split_name}_Mixture\": met(y, predMix),\n",
        "    }\n",
        "\n",
        "    pred_df = d[[\"Material and Incubation day\",\"BaseMaterial\",\"DayInt\",\"condition\",TARGET]].copy()\n",
        "    pred_df[\"pred_expertA\"] = predA\n",
        "    pred_df[\"pred_expertB\"] = predB\n",
        "    pred_df[\"gate_wB\"] = wB\n",
        "    pred_df[\"pred_mixture\"] = predMix\n",
        "    return metrics, pred_df\n",
        "\n",
        "df_full = load_clean_df()\n",
        "\n",
        "all_metrics = {}\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df_full,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "moe_random = fit_moe(train_df)\n",
        "m_train, pred_train = eval_moe(moe_random, train_df, \"randomTrain\")\n",
        "m_test, pred_test = eval_moe(moe_random, test_df, \"randomTest\")\n",
        "\n",
        "all_metrics.update(m_train)\n",
        "all_metrics.update(m_test)\n",
        "\n",
        "pred_test.to_csv(OUTDIR / \"moe_blind_predictions_random_split.csv\", index=False)\n",
        "\n",
        "ground_df = df_full[df_full[\"condition\"]==\"ground\"].copy()\n",
        "flight_df = df_full[df_full[\"condition\"]==\"flight\"].copy()\n",
        "\n",
        "moe_g2f = fit_moe(ground_df)\n",
        "m_ground_in, pred_ground_in = eval_moe(moe_g2f, ground_df, \"G2F_train_ground\")\n",
        "m_flight_blind, pred_flight_blind = eval_moe(moe_g2f, flight_df, \"G2F_test_flight\")\n",
        "\n",
        "all_metrics.update(m_ground_in)\n",
        "all_metrics.update(m_flight_blind)\n",
        "\n",
        "pred_flight_blind.to_csv(OUTDIR / \"moe_blind_predictions_ground_to_flight.csv\", index=False)\n",
        "\n",
        "json.dump(all_metrics, open(OUTDIR / \"moe_blind_metrics.json\",\"w\"), indent=2)\n",
        "\n",
        "def pretty(m):\n",
        "    out = {}\n",
        "    for k,v in m.items():\n",
        "        out[k] = {kk: (round(vv,4) if isinstance(vv,float) else vv) for kk,vv in v.items()}\n",
        "    return out\n",
        "\n",
        "print(\"Blind-test metrics (rounded):\")\n",
        "print(json.dumps(pretty(all_metrics), indent=2))\n",
        "print(\"\\nWrote:\")\n",
        "print(\"-\", OUTDIR / \"moe_blind_metrics.json\")\n",
        "print(\"-\", OUTDIR / \"moe_blind_predictions_random_split.csv\")\n",
        "print(\"-\", OUTDIR / \"moe_blind_predictions_ground_to_flight.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV2GuHbdGSP-",
        "outputId": "583b7661-2c73-48af-9dbb-ac4d52afa011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consolidated duplicate columns: [('Biofilm mass (µm^3/µm^2)', 2)]\n",
            "Done. Outputs in: /content/lsds55_outputs_expertA\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "expert A only pipeline\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# ------------------------- Config -------------------------\n",
        "EXCEL_PATH = Path(\"/content/LSDS-55_microscopy_LSDS-55_ConfocalMicroscopy_flores_SUBMITTED.xlsx\")\n",
        "OUTDIR = Path(\"/content/lsds55_outputs_expertA\")\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "EXPECTED_FEATURES = [\n",
        "    \"Biofilm mass (µm^3/µm^2)\",\n",
        "    \"Biofilm biomass mean thickness (µm)\",\n",
        "    \"Biofilm Maximum thickness (µm)\",\n",
        "    \"roughness coefficient Ra*\",\n",
        "]\n",
        "EXPECTED_COLS = [\n",
        "    \"Material and Incubation day\",\n",
        "    \"sample ID\",\n",
        "    *EXPECTED_FEATURES,\n",
        "    TARGET,\n",
        "]\n",
        "\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = [re.sub(r\"\\s+\", \" \", str(c)).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def normalize_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    df = clean_cols(df)\n",
        "    rename_map = {}\n",
        "    for col in df.columns:\n",
        "        c = col.lower()\n",
        "        if \"material\" in c and \"incubation\" in c:\n",
        "            rename_map[col] = \"Material and Incubation day\"\n",
        "        elif c.startswith(\"sample\"):\n",
        "            rename_map[col] = \"sample ID\"\n",
        "        elif \"mass\" in c and (\"um\" in c or \"µm\" in c):\n",
        "            rename_map[col] = \"Biofilm mass (µm^3/µm^2)\"\n",
        "        elif \"mean thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm biomass mean thickness (µm)\"\n",
        "        elif \"maximum thickness\" in c:\n",
        "            rename_map[col] = \"Biofilm Maximum thickness (µm)\"\n",
        "        elif \"surface area coverage\" in c:\n",
        "            rename_map[col] = \"Biofilm surface area coverage (%)\"\n",
        "        elif \"roughness\" in c:\n",
        "            rename_map[col] = \"roughness coefficient Ra*\"\n",
        "    df = df.rename(columns=rename_map)\n",
        "    keep = [c for c in EXPECTED_COLS if c in df.columns]\n",
        "    return df[keep] if keep else df\n",
        "\n",
        "def parse_material_day(s: str):\n",
        "    if pd.isna(s):\n",
        "        return (np.nan, np.nan)\n",
        "    s_norm = re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
        "    m_day = re.search(r\"day\\s*([123])\", s_norm, flags=re.IGNORECASE)\n",
        "    day = int(m_day.group(1)) if m_day else np.nan\n",
        "    mat = re.split(r\"\\bday\\b\", s_norm, flags=re.IGNORECASE)[0].strip()\n",
        "    mat = mat.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (mat, day)\n",
        "\n",
        "def parse_base_and_day_v2(x):\n",
        "\n",
        "    if pd.isna(x):\n",
        "        return (np.nan, np.nan)\n",
        "    s = re.sub(r\"\\s+\", \" \", str(x)).strip()\n",
        "    m = re.search(r\"day\\s*([123])\\s*$\", s, flags=re.IGNORECASE)\n",
        "    day = int(m.group(1)) if m else np.nan\n",
        "    base = re.sub(r\"day\\s*[123]\\s*$\", \"\", s, flags=re.IGNORECASE).strip()\n",
        "    base = base.replace(\"Celullose\", \"Cellulose\").strip()\n",
        "    return (base, day)\n",
        "\n",
        "def consolidate_duplicate_columns(df: pd.DataFrame, cols_to_merge: list) -> pd.DataFrame:\n",
        "\n",
        "    df = df.copy()\n",
        "    merged_info = []\n",
        "    for name in cols_to_merge:\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) <= 1:\n",
        "            continue\n",
        "        dup_block = df.iloc[:, idxs]\n",
        "        dup_block_num = dup_block.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        merged_series = dup_block_num.bfill(axis=1).iloc[:, 0]\n",
        "        if merged_series.isna().all():\n",
        "            merged_series = dup_block.bfill(axis=1).iloc[:, 0]\n",
        "        first_idx = idxs[0]\n",
        "        df.iloc[:, first_idx] = merged_series\n",
        "        df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        merged_info.append((name, len(idxs)))\n",
        "    if merged_info:\n",
        "        print(\"Consolidated duplicate columns:\", merged_info)\n",
        "    return df\n",
        "\n",
        "def safe_coerce_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Coerce listed columns to numeric, dropping extra duplicates if any remain.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for name in cols:\n",
        "        if name not in df.columns:\n",
        "            continue\n",
        "        idxs = np.where(df.columns.values == name)[0]\n",
        "        if len(idxs) > 1:\n",
        "            df.drop(columns=df.columns[idxs[1:]], inplace=True)\n",
        "        df[name] = pd.to_numeric(df[name], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true) == 0 or len(y_pred) == 0:\n",
        "        return {\"note\": \"no rows\"}\n",
        "    out = {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "    }\n",
        "    out[\"r2\"] = float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan\n",
        "    out[\"n\"] = int(len(y_true))\n",
        "    return out\n",
        "\n",
        "# load and clean\n",
        "xl = pd.ExcelFile(EXCEL_PATH)\n",
        "dfs = []\n",
        "if \"GROUND DATA\" in xl.sheet_names:\n",
        "    g = pd.read_excel(EXCEL_PATH, sheet_name=\"GROUND DATA\")\n",
        "    g = normalize_headers(g).dropna(how=\"all\")\n",
        "    g[\"condition\"] = \"ground\"\n",
        "    dfs.append(g)\n",
        "\n",
        "if \"FLIGHT DATA\" in xl.sheet_names:\n",
        "    f = pd.read_excel(EXCEL_PATH, sheet_name=\"FLIGHT DATA\")\n",
        "    f = normalize_headers(f).dropna(how=\"all\")\n",
        "    f[\"condition\"] = \"flight\"\n",
        "    dfs.append(f)\n",
        "\n",
        "if not dfs:\n",
        "    raise RuntimeError(\"Neither 'GROUND DATA' nor 'FLIGHT DATA' sheets found.\")\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = clean_cols(df_all)\n",
        "\n",
        "# Consolidate duplicates before coercion\n",
        "cols_for_merge = list(set(EXPECTED_COLS + EXPECTED_FEATURES + [TARGET]))\n",
        "df_all = consolidate_duplicate_columns(df_all, cols_to_merge=cols_for_merge)\n",
        "\n",
        "# Parse Material & day + BaseMaterial & DayInt\n",
        "if \"Material and Incubation day\" not in df_all.columns:\n",
        "    raise RuntimeError(\"Missing 'Material and Incubation day' after header normalization/merge.\")\n",
        "df_all[[\"Material\", \"day\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_material_day(s)))\n",
        "df_all[[\"BaseMaterial\", \"DayInt\"]] = df_all[\"Material and Incubation day\"].apply(lambda s: pd.Series(parse_base_and_day_v2(s)))\n",
        "\n",
        "# Coerce numerics\n",
        "df_all = safe_coerce_numeric(df_all, EXPECTED_FEATURES + [TARGET])\n",
        "\n",
        "# Keep essentials\n",
        "df = df_all.dropna(subset=[\"BaseMaterial\", \"DayInt\", \"condition\", TARGET]).copy()\n",
        "df[\"DayInt\"] = pd.to_numeric(df[\"DayInt\"], errors=\"coerce\")\n",
        "df[\"condition_bin\"] = (df[\"condition\"] == \"flight\").astype(int)\n",
        "df[\"DayInt_x_condition\"] = df[\"DayInt\"] * df[\"condition_bin\"]\n",
        "\n",
        "df.to_csv(OUTDIR / \"cleaned_combined.csv\", index=False)\n",
        "\n",
        "existing_feats = [c for c in EXPECTED_FEATURES if c in df.columns]\n",
        "missing_feats = [c for c in EXPECTED_FEATURES if c not in df.columns]\n",
        "\n",
        "if len(existing_feats) < len(EXPECTED_FEATURES):\n",
        "    numeric_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    fallback_exclude = {TARGET, \"day\", \"DayInt\", \"condition_bin\", \"DayInt_x_condition\"}\n",
        "    fallback_feats = [c for c in numeric_candidates if c not in fallback_exclude]\n",
        "    feature_cols = existing_feats + [c for c in fallback_feats if c not in existing_feats]\n",
        "else:\n",
        "    feature_cols = existing_feats\n",
        "\n",
        "feature_cols = [c for c in feature_cols if df[c].notna().any()]\n",
        "\n",
        "summary = {\n",
        "    \"rows_total\": int(len(df_all)),\n",
        "    \"rows_used\": int(len(df)),\n",
        "    \"conditions\": df[\"condition\"].value_counts(dropna=False).to_dict(),\n",
        "    \"base_materials\": df[\"BaseMaterial\"].value_counts().to_dict(),\n",
        "    \"days\": df[\"DayInt\"].value_counts().sort_index().to_dict(),\n",
        "    \"features_used_for_G2F\": feature_cols,\n",
        "    \"features_missing_from_expected\": missing_feats,\n",
        "}\n",
        "with open(OUTDIR / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "metrics = {}\n",
        "\n",
        "if (\"ground\" in df[\"condition\"].unique()) and (\"flight\" in df[\"condition\"].unique()) and len(feature_cols) > 0:\n",
        "    df_g = df[(df[\"condition\"] == \"ground\") & df[TARGET].notna()]\n",
        "    df_f = df[(df[\"condition\"] == \"flight\") & df[TARGET].notna()]\n",
        "\n",
        "    if not df_g.empty and not df_f.empty:\n",
        "        X_train = df_g[feature_cols].values\n",
        "        y_train = df_g[TARGET].values\n",
        "        X_test = df_f[feature_cols].values\n",
        "        y_test = df_f[TARGET].values\n",
        "\n",
        "        rf_g2f = RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1)\n",
        "        rf_g2f.fit(X_train, y_train)\n",
        "        y_pred_f = rf_g2f.predict(X_test)\n",
        "\n",
        "        metrics[\"g2f_regression\"] = met(y_test, y_pred_f)\n",
        "        metrics[\"g2f_regression\"].update({\n",
        "            \"n_train_ground\": int(len(df_g)),\n",
        "            \"n_test_flight\": int(len(df_f)),\n",
        "            \"n_features\": int(len(feature_cols)),\n",
        "        })\n",
        "\n",
        "        pred_df = df_f.copy()\n",
        "        pred_df[\"predicted_coverage_g2f\"] = y_pred_f\n",
        "        pred_df.to_csv(OUTDIR / \"g2f_regression_predictions.csv\", index=False)\n",
        "\n",
        "        # Plot pred vs actual\n",
        "        plt.figure()\n",
        "        plt.scatter(y_test, y_pred_f)\n",
        "        plt.xlabel(\"Actual Flight Coverage\")\n",
        "        plt.ylabel(\"Predicted Flight Coverage (trained on Ground)\")\n",
        "        plt.title(\"Ground→Flight coverage regression (Expert A RF)\")\n",
        "        lo = float(min(y_test.min(), y_pred_f.min()))\n",
        "        hi = float(max(y_test.max(), y_pred_f.max()))\n",
        "        plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(OUTDIR / \"g2f_pred_vs_actual.png\", dpi=150)\n",
        "        plt.close()\n",
        "    else:\n",
        "        metrics[\"g2f_regression\"] = {\"note\": \"Insufficient ground/flight rows with target.\"}\n",
        "else:\n",
        "    metrics[\"g2f_regression\"] = {\"note\": \"Require both ground and flight and usable features.\"}\n",
        "\n",
        "# Numeric features: all numerics except TARGET\n",
        "num_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numA = [c for c in num_candidates if c != TARGET]\n",
        "# Categorical: BaseMaterial and condition\n",
        "catA = [c for c in [\"BaseMaterial\", \"condition\"] if c in df.columns]\n",
        "\n",
        "df_dom = df[df[TARGET].notna()].copy()\n",
        "\n",
        "if not df_dom.empty and (len(numA) + len(catA) > 0):\n",
        "    preA = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(with_mean=False), numA),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catA),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "    rf_base = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "    rf_grid = {\n",
        "        \"model__max_depth\": [5, 10, None],\n",
        "        \"model__min_samples_leaf\": [1, 2, 4],\n",
        "        \"model__max_features\": [None, \"sqrt\"],\n",
        "    }\n",
        "    pipeA = Pipeline([(\"pre\", preA), (\"model\", rf_base)])\n",
        "    cvA = GridSearchCV(\n",
        "        pipeA,\n",
        "        rf_grid,\n",
        "        cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        scoring=\"r2\",\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    cvA.fit(df_dom[numA + catA], df_dom[TARGET])\n",
        "    df_dom[\"pred_expertA\"] = cvA.predict(df_dom[numA + catA])\n",
        "    df_dom.to_csv(OUTDIR / \"domain_aware_predictions_expertA.csv\", index=False)\n",
        "\n",
        "    for cond in [\"ground\", \"flight\"]:\n",
        "        sub = df_dom[df_dom[\"condition\"] == cond]\n",
        "        if not sub.empty:\n",
        "            m = met(sub[TARGET].values, sub[\"pred_expertA\"].values)\n",
        "            m[\"cv_best_params\"] = {k: v for k, v in cvA.best_params_.items()}\n",
        "            metrics[f\"domain_model_base_{cond}\"] = m\n",
        "        else:\n",
        "            metrics[f\"domain_model_base_{cond}\"] = {\"note\": \"no rows\"}\n",
        "else:\n",
        "    metrics[\"domain_model_base\"] = {\"note\": \"No rows or no usable features for domain-aware model.\"}\n",
        "\n",
        "plots_dir = OUTDIR / \"trajectories_base\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "traj_rows = []\n",
        "have_flight = \"flight\" in df[\"condition\"].unique()\n",
        "materials = sorted(df[\"BaseMaterial\"].dropna().unique())\n",
        "\n",
        "# Fix a global y-axis range for ALL trajectory plots\n",
        "Y_MIN, Y_MAX = 0.0, 100.0  # coverage is a percentage; keep it standardized\n",
        "\n",
        "for mat in materials:\n",
        "    g_sub = df[(df[\"condition\"] == \"ground\") & (df[\"BaseMaterial\"] == mat)]\n",
        "    f_sub = df[(df[\"condition\"] == \"flight\") & (df[\"BaseMaterial\"] == mat)] if have_flight else pd.DataFrame()\n",
        "\n",
        "    # Need ≥2 ground days to fit slope\n",
        "    if g_sub[\"DayInt\"].nunique() < 2:\n",
        "        continue\n",
        "\n",
        "    # Linear ground trajectory: coverage ~ DayInt\n",
        "    Xg = g_sub[[\"DayInt\"]].values\n",
        "    yg = g_sub[TARGET].values\n",
        "    lin = LinearRegression().fit(Xg, yg)\n",
        "\n",
        "    slope = float(lin.coef_[0])\n",
        "    intercept = float(lin.intercept_)\n",
        "\n",
        "    # Ground per-day growth rate if Day1 and Day3 exist\n",
        "    growth_rate = np.nan\n",
        "    if (g_sub[\"DayInt\"] == 1).any() and (g_sub[\"DayInt\"] == 3).any():\n",
        "        cov_d1 = g_sub.loc[g_sub[\"DayInt\"] == 1, TARGET].mean()\n",
        "        cov_d3 = g_sub.loc[g_sub[\"DayInt\"] == 3, TARGET].mean()\n",
        "        growth_rate = (cov_d3 - cov_d1) / 2.0\n",
        "\n",
        "    # Method A: ground line evaluated at Day 3\n",
        "    pred_day3_A = float(lin.predict(np.array([[3]]))[0])\n",
        "\n",
        "    # Method B: Flight Day1 + ground growth_rate * 2 (if available)\n",
        "    pred_day3_B = np.nan\n",
        "    actual_flight_d3 = np.nan\n",
        "    if have_flight and not f_sub.empty:\n",
        "        f_d1 = f_sub[f_sub[\"DayInt\"] == 1]\n",
        "        f_d3 = f_sub[f_sub[\"DayInt\"] == 3]\n",
        "        if not f_d3.empty:\n",
        "            actual_flight_d3 = float(f_d3[TARGET].mean())\n",
        "        if not f_d1.empty and not np.isnan(growth_rate):\n",
        "            pred_day3_B = float(f_d1[TARGET].mean() + 2.0 * growth_rate)\n",
        "\n",
        "    traj_rows.append({\n",
        "        \"BaseMaterial\": mat,\n",
        "        \"ground_lin_slope\": slope,\n",
        "        \"ground_lin_intercept\": intercept,\n",
        "        \"ground_growth_rate_per_day\": growth_rate,\n",
        "        \"predicted_flight_day3_methodA_ground_line\": pred_day3_A,\n",
        "        \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\": pred_day3_B,\n",
        "        \"actual_flight_day3\": actual_flight_d3,\n",
        "    })\n",
        "\n",
        "    plt.figure()\n",
        "    # Ground points\n",
        "    plt.scatter(g_sub[\"DayInt\"], g_sub[TARGET], label=\"Ground (actual)\", color=\"tab:blue\")\n",
        "    # Ground linear fit\n",
        "    xs = np.array([[1], [2], [3]])\n",
        "    ys = lin.predict(xs)\n",
        "    plt.plot(xs.flatten(), ys, linestyle=\"--\", label=\"Ground fit\", color=\"tab:blue\")\n",
        "\n",
        "    # Flight points + predictions if present\n",
        "    if have_flight and not f_sub.empty:\n",
        "        plt.scatter(f_sub[\"DayInt\"], f_sub[TARGET], label=\"Flight (actual)\", color=\"tab:orange\")\n",
        "        if not np.isnan(pred_day3_B):\n",
        "            plt.scatter([3], [pred_day3_B], marker=\"x\", label=\"Pred Flight D3 (Method B)\", color=\"tab:green\")\n",
        "        plt.scatter([3], [pred_day3_A], marker=\"^\", label=\"Pred Flight D3 (Method A)\", color=\"tab:red\")\n",
        "\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Biofilm surface area coverage (%)\")\n",
        "    plt.title(f\"Trajectory (BaseMaterial): {mat}\")\n",
        "\n",
        "    # Standardized axes\n",
        "    plt.xlim(0.8, 3.2)\n",
        "    plt.xticks([1, 2, 3])\n",
        "    plt.ylim(Y_MIN, Y_MAX)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    safe_name = re.sub(r\"[^A-Za-z0-9]+\", \"_\", mat).strip(\"_\")\n",
        "    plt.savefig(plots_dir / f\"trajectory_{safe_name}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "traj_df = pd.DataFrame(traj_rows)\n",
        "traj_df.to_csv(OUTDIR / \"trajectory_predictions_base_material.csv\", index=False)\n",
        "\n",
        "def eval_block(df_eval, pred_col):\n",
        "    if df_eval.empty or pred_col not in df_eval.columns:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    mask = (~df_eval[\"actual_flight_day3\"].isna()) & (~df_eval[pred_col].isna())\n",
        "    y_true = df_eval.loc[mask, \"actual_flight_day3\"].values\n",
        "    y_pred = df_eval.loc[mask, pred_col].values\n",
        "    if len(y_true) == 0:\n",
        "        return {\"note\": \"no comparable rows\"}\n",
        "    return met(y_true, y_pred)\n",
        "\n",
        "metrics[\"trajectory_base_methodA\"] = eval_block(traj_df, \"predicted_flight_day3_methodA_ground_line\")\n",
        "metrics[\"trajectory_base_methodB\"] = eval_block(traj_df, \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\")\n",
        "\n",
        "def fit_expertA_domain(train_df):\n",
        "    \"\"\"Fit CV-tuned RF (Expert A) on a given training df.\"\"\"\n",
        "    df_tr = train_df.copy()\n",
        "    num_candidates = df_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numA_local = [c for c in num_candidates if c != TARGET]\n",
        "    catA_local = [c for c in [\"BaseMaterial\", \"condition\"] if c in df_tr.columns]\n",
        "    preA_local = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(with_mean=False), numA_local),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), catA_local),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "    rf_base_local = RandomForestRegressor(n_estimators=600, random_state=42, n_jobs=-1)\n",
        "    rf_grid_local = {\n",
        "        \"model__max_depth\": [5, 10, None],\n",
        "        \"model__min_samples_leaf\": [1, 2, 4],\n",
        "        \"model__max_features\": [None, \"sqrt\"],\n",
        "    }\n",
        "    pipeA_local = Pipeline([(\"pre\", preA_local), (\"model\", rf_base_local)])\n",
        "    cv_local = GridSearchCV(\n",
        "        pipeA_local,\n",
        "        rf_grid_local,\n",
        "        cv=KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "        scoring=\"r2\",\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    X_tr = df_tr[numA_local + catA_local]\n",
        "    y_tr = df_tr[TARGET].values\n",
        "    cv_local.fit(X_tr, y_tr)\n",
        "    return cv_local, numA_local, catA_local\n",
        "\n",
        "def eval_expertA_domain(model, num_cols, cat_cols, df_eval):\n",
        "    d = df_eval.copy()\n",
        "    X = d[num_cols + cat_cols]\n",
        "    y = d[TARGET].values\n",
        "    y_pred = model.predict(X)\n",
        "    return met(y, y_pred), y_pred\n",
        "\n",
        "# random 70/30 split\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "model_rand, num_rand, cat_rand = fit_expertA_domain(train_df)\n",
        "m_train_rand, y_pred_train = eval_expertA_domain(model_rand, num_rand, cat_rand, train_df)\n",
        "m_test_rand, y_pred_test = eval_expertA_domain(model_rand, num_rand, cat_rand, test_df)\n",
        "\n",
        "metrics[\"expertA_randomTrain\"] = m_train_rand\n",
        "metrics[\"expertA_randomTest\"] = m_test_rand\n",
        "\n",
        "pred_rand = test_df[[\"Material and Incubation day\", \"BaseMaterial\", \"DayInt\", \"condition\", TARGET]].copy()\n",
        "pred_rand[\"pred_expertA_blind_random\"] = y_pred_test\n",
        "pred_rand.to_csv(OUTDIR / \"expertA_blind_predictions_random_split.csv\", index=False)\n",
        "\n",
        "# 2) Ground→Flight blind test (train on ground only, test on flight)\n",
        "ground_df = df[df[\"condition\"] == \"ground\"].copy()\n",
        "flight_df = df[df[\"condition\"] == \"flight\"].copy()\n",
        "\n",
        "if not ground_df.empty and not flight_df.empty:\n",
        "    model_g2f, num_g2f, cat_g2f = fit_expertA_domain(ground_df)\n",
        "    m_ground_in, y_pred_ground = eval_expertA_domain(model_g2f, num_g2f, cat_g2f, ground_df)\n",
        "    m_flight_blind, y_pred_flight = eval_expertA_domain(model_g2f, num_g2f, cat_g2f, flight_df)\n",
        "\n",
        "    metrics[\"expertA_G2F_train_ground\"] = m_ground_in\n",
        "    metrics[\"expertA_G2F_test_flight\"] = m_flight_blind\n",
        "\n",
        "    pred_g2f = flight_df[[\"Material and Incubation day\", \"BaseMaterial\", \"DayInt\", \"condition\", TARGET]].copy()\n",
        "    pred_g2f[\"pred_expertA_blind_G2F\"] = y_pred_flight\n",
        "    pred_g2f.to_csv(OUTDIR / \"expertA_blind_predictions_G2F.csv\", index=False)\n",
        "else:\n",
        "    metrics[\"expertA_G2F\"] = {\"note\": \"Need both ground and flight rows for G→F blind test.\"}\n",
        "\n",
        "with open(OUTDIR / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "with open(OUTDIR / \"README.txt\", \"w\") as f:\n",
        "    f.write(\n",
        "f\"\"\"Outputs: {OUTDIR}\n",
        "\"\"\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aopCzJhSGTF1",
        "outputId": "c1f530b5-5148-4112-e444-815b6d1103de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Expert A params: {'model__max_depth': 3, 'model__max_features': None, 'model__min_samples_leaf': 2}\n",
            "Done. Expert A trajectory plots saved in: /content/lsds55_outputs_expertA/trajectories_expertA\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "OUTDIR = Path(\"/content/lsds55_outputs_expertA\")\n",
        "TARGET = \"Biofilm surface area coverage (%)\"\n",
        "CLEAN_CSV = OUTDIR / \"cleaned_combined.csv\"\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    if len(y_true) == 0:\n",
        "        return {\"note\": \"no rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\": float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"n\": int(len(y_true)),\n",
        "    }\n",
        "\n",
        "df = pd.read_csv(CLEAN_CSV)\n",
        "\n",
        "# Make sure required columns exist; if not, stop early\n",
        "required_cols = {\"BaseMaterial\", \"DayInt\", \"condition\", TARGET}\n",
        "missing = required_cols - set(df.columns)\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required columns in cleaned_combined.csv: {missing}\")\n",
        "\n",
        "# Drop rows without essential info\n",
        "df = df.dropna(subset=[\"BaseMaterial\", \"DayInt\", \"condition\", TARGET]).copy()\n",
        "df[\"condition_bin\"] = (df[\"condition\"] == \"flight\").astype(int)\n",
        "\n",
        "# Numeric + categorical setup\n",
        "num_candidates = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if TARGET in num_candidates:\n",
        "    num_candidates.remove(TARGET)\n",
        "\n",
        "# Keep DayInt and condition_bin explicitly at the end for readability\n",
        "num_cols = [c for c in num_candidates if c not in [\"DayInt\", \"condition_bin\"]]\n",
        "for c in [\"DayInt\", \"condition_bin\"]:\n",
        "    if c in df.columns and c not in num_cols:\n",
        "        num_cols.append(c)\n",
        "\n",
        "cat_cols = [c for c in [\"BaseMaterial\", \"condition\"] if c in df.columns]\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "rf_base = RandomForestRegressor(random_state=42, n_estimators=500, n_jobs=-1)\n",
        "\n",
        "param_grid = {\n",
        "    \"model__max_depth\": [3, 5, None],\n",
        "    \"model__min_samples_leaf\": [1, 2, 4],\n",
        "    \"model__max_features\": [None],\n",
        "}\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"model\", rf_base)])\n",
        "cv = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
        "X_all = df[num_cols + cat_cols]\n",
        "y_all = df[TARGET].values\n",
        "\n",
        "cv.fit(X_all, y_all)\n",
        "best_model = cv.best_estimator_\n",
        "print(\"Best Expert A params:\", cv.best_params_)\n",
        "\n",
        "# Add Expert A predictions to df\n",
        "df[\"expertA_pred\"] = best_model.predict(X_all)\n",
        "\n",
        "plots_dir = OUTDIR / \"trajectories_expertA\"\n",
        "plots_dir.mkdir(exist_ok=True)\n",
        "\n",
        "Y_MIN, Y_MAX = 0.0, 100.0  # standardized axis for coverage\n",
        "materials = sorted(df[\"BaseMaterial\"].dropna().unique())\n",
        "\n",
        "traj_rows = []\n",
        "\n",
        "for mat in materials:\n",
        "    mat_sub = df[df[\"BaseMaterial\"] == mat].copy()\n",
        "    g_sub = mat_sub[mat_sub[\"condition\"] == \"ground\"]\n",
        "    f_sub = mat_sub[mat_sub[\"condition\"] == \"flight\"]\n",
        "\n",
        "    # Need >=2 ground days for linear trajectory\n",
        "    if g_sub[\"DayInt\"].nunique() < 2:\n",
        "        continue\n",
        "\n",
        "    Xg = g_sub[[\"DayInt\"]].values\n",
        "    yg = g_sub[TARGET].values\n",
        "    lin = LinearRegression().fit(Xg, yg)\n",
        "\n",
        "    slope = float(lin.coef_[0])\n",
        "    intercept = float(lin.intercept_)\n",
        "\n",
        "    # Ground growth rate (Day1 -> Day3) for Method B\n",
        "    growth_rate = np.nan\n",
        "    if (g_sub[\"DayInt\"] == 1).any() and (g_sub[\"DayInt\"] == 3).any():\n",
        "        cov_d1 = g_sub.loc[g_sub[\"DayInt\"] == 1, TARGET].mean()\n",
        "        cov_d3 = g_sub.loc[g_sub[\"DayInt\"] == 3, TARGET].mean()\n",
        "        growth_rate = (cov_d3 - cov_d1) / 2.0\n",
        "\n",
        "    pred_day3_A = float(lin.predict(np.array([[3]]))[0])\n",
        "\n",
        "    pred_day3_B = np.nan\n",
        "    actual_flight_d3 = np.nan\n",
        "    if not f_sub.empty:\n",
        "        f_d1 = f_sub[f_sub[\"DayInt\"] == 1]\n",
        "        f_d3 = f_sub[f_sub[\"DayInt\"] == 3]\n",
        "        if not f_d3.empty:\n",
        "            actual_flight_d3 = float(f_d3[TARGET].mean())\n",
        "        if not f_d1.empty and not np.isnan(growth_rate):\n",
        "            pred_day3_B = float(f_d1[TARGET].mean() + 2.0 * growth_rate)\n",
        "\n",
        "    traj_rows.append({\n",
        "        \"BaseMaterial\": mat,\n",
        "        \"ground_lin_slope\": slope,\n",
        "        \"ground_lin_intercept\": intercept,\n",
        "        \"ground_growth_rate_per_day\": growth_rate,\n",
        "        \"predicted_flight_day3_methodA_ground_line\": pred_day3_A,\n",
        "        \"predicted_flight_day3_methodB_flightD1_plus_ground_rate\": pred_day3_B,\n",
        "        \"actual_flight_day3\": actual_flight_d3,\n",
        "    })\n",
        "\n",
        "    # Ground\n",
        "    if not g_sub.empty:\n",
        "        g_mean = (\n",
        "            g_sub.groupby(\"DayInt\")\n",
        "            .agg(actual=(TARGET, \"mean\"), pred=(\"expertA_pred\", \"mean\"))\n",
        "            .reset_index()\n",
        "        )\n",
        "    else:\n",
        "        g_mean = None\n",
        "\n",
        "    # Flight\n",
        "    if not f_sub.empty:\n",
        "        f_mean = (\n",
        "            f_sub.groupby(\"DayInt\")\n",
        "            .agg(actual=(TARGET, \"mean\"), pred=(\"expertA_pred\", \"mean\"))\n",
        "            .reset_index()\n",
        "        )\n",
        "    else:\n",
        "        f_mean = None\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    # Ground actual scatter\n",
        "    plt.scatter(g_sub[\"DayInt\"], g_sub[TARGET],\n",
        "                label=\"Ground (actual)\", color=\"tab:blue\")\n",
        "\n",
        "    # Ground linear fit (old \"Ground fit\")\n",
        "    xs = np.array([[1], [2], [3]])\n",
        "    ys = lin.predict(xs)\n",
        "    plt.plot(xs.flatten(), ys, linestyle=\"--\",\n",
        "             label=\"Ground fit (linear)\", color=\"tab:blue\")\n",
        "\n",
        "    # Flight actual scatter\n",
        "    if not f_sub.empty:\n",
        "        plt.scatter(f_sub[\"DayInt\"], f_sub[TARGET],\n",
        "                    label=\"Flight (actual)\", color=\"tab:orange\")\n",
        "\n",
        "    # Old trajectory predictions (Method A & B)\n",
        "    if not np.isnan(pred_day3_B):\n",
        "        plt.scatter([3], [pred_day3_B], marker=\"x\",\n",
        "                    label=\"Pred Flight D3 (Method B)\", color=\"tab:green\")\n",
        "    plt.scatter([3], [pred_day3_A], marker=\"^\",\n",
        "                label=\"Pred Flight D3 (Method A)\", color=\"tab:red\")\n",
        "\n",
        "    # Expert A ground predictions (day-wise mean)\n",
        "    if g_mean is not None and len(g_mean) > 0:\n",
        "        plt.plot(\n",
        "            g_mean[\"DayInt\"],\n",
        "            g_mean[\"pred\"],\n",
        "            \"-.\",\n",
        "            color=\"tab:purple\",\n",
        "            label=\"Ground Expert A (mean pred)\",\n",
        "        )\n",
        "\n",
        "    # Expert A flight predictions (day-wise mean)\n",
        "    if f_mean is not None and len(f_mean) > 0:\n",
        "        plt.plot(\n",
        "            f_mean[\"DayInt\"],\n",
        "            f_mean[\"pred\"],\n",
        "            \"-.\",\n",
        "            color=\"tab:brown\",\n",
        "            label=\"Flight Expert A (mean pred)\",\n",
        "        )\n",
        "\n",
        "    plt.xlabel(\"Day\")\n",
        "    plt.ylabel(\"Biofilm surface area coverage (%)\")\n",
        "    plt.title(f\"Trajectory with Expert A (BaseMaterial): {mat}\")\n",
        "    plt.xlim(0.8, 3.2)\n",
        "    plt.xticks([1, 2, 3])\n",
        "    plt.ylim(Y_MIN, Y_MAX)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    safe_name = \"\".join(ch if ch.isalnum() else \"_\" for ch in mat).strip(\"_\")\n",
        "    plt.savefig(plots_dir / f\"trajectory_expertA_{safe_name}.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "traj_df = pd.DataFrame(traj_rows)\n",
        "traj_df.to_csv(plots_dir / \"trajectory_expertA_summary.csv\", index=False)\n",
        "\n",
        "print(f\"Done. Expert A trajectory plots saved in: {plots_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NEW Code added below, aligning with Method A of the Current Results in the project design paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calibration fitted: y_true ≈ a + b * y_pred\n",
            "a = -20.43978786518256    b = 2.2410182636970144\n",
            "Done.\n",
            "Pairs total: 18\n",
            "Train pairs: 12  Test pairs: 6\n",
            "Metrics written to: lsds55_outputs_strict_g2f\\metrics.json\n",
            "Predictions CSV: lsds55_outputs_strict_g2f\\strict_ground_to_flight_predictions.csv\n",
            "Pred vs Actual plot: lsds55_outputs_strict_g2f\\strict_ground_to_flight_pred_vs_actual.png\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "This script is a little different from the above ones in that it trains on ground-flight data and tests using only ground inputs instead of flight inputs.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "CSV_PATH = Path(\"cleaned_combined_updated.csv\")   \n",
        "OUTDIR = Path(\"lsds55_outputs_strict_g2f\") \n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TARGET = \"Biofilm surface area coverage (%)\"  # flight y-label\n",
        "COL_MAX_THICK = \"Biofilm Maximum thickness (µm)\"\n",
        "COL_ROUGH_BIO = \"roughness coefficient Ra*\"\n",
        "COL_MAT_ROUGH = \"Material_Roughness\"\n",
        "COL_CA = \"Contact_Angle\"\n",
        "COL_COSTH = \"Cos(theta)\"\n",
        "COL_WADH = \"Wadh\"\n",
        "\n",
        "\n",
        "def met(y_true, y_pred):\n",
        "    \"\"\"Compute RMSE/MAE/R2 + n.\"\"\"\n",
        "    if len(y_true) == 0:\n",
        "        return {\"note\": \"no rows\"}\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\": float(r2_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else np.nan,\n",
        "        \"n\": int(len(y_true)),\n",
        "    }\n",
        "\n",
        "\n",
        "if not CSV_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Cannot find {CSV_PATH}. Make sure cleaned_combined.csv is next to this script.\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "required_cols = [\n",
        "    \"BaseMaterial\",\n",
        "    \"DayInt\",\n",
        "    \"condition\",\n",
        "    TARGET,\n",
        "    COL_MAX_THICK,\n",
        "    COL_ROUGH_BIO,\n",
        "    COL_MAT_ROUGH,\n",
        "    COL_CA,\n",
        "    COL_COSTH,\n",
        "    COL_WADH,\n",
        "]\n",
        "\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required columns in cleaned_combined.csv: {missing}\")\n",
        "\n",
        "df[\"DayInt\"] = pd.to_numeric(df[\"DayInt\"], errors=\"coerce\")\n",
        "df = df.dropna(subset=[\"BaseMaterial\", \"DayInt\", \"condition\", TARGET]).copy()\n",
        "\n",
        "\n",
        "# Separate ground and flight\n",
        "df_ground = df[df[\"condition\"].str.lower() == \"ground\"].copy()\n",
        "df_flight = df[df[\"condition\"].str.lower() == \"flight\"].copy()\n",
        "\n",
        "group_cols = [\"BaseMaterial\", \"DayInt\"]\n",
        "\n",
        "g_groups = df_ground.groupby(group_cols)\n",
        "f_groups = df_flight.groupby(group_cols)\n",
        "\n",
        "common_keys = sorted(set(g_groups.groups.keys()) & set(f_groups.groups.keys()))\n",
        "\n",
        "paired_rows = []\n",
        "\n",
        "for (mat, day) in common_keys:\n",
        "    g_block = g_groups.get_group((mat, day))\n",
        "    f_block = f_groups.get_group((mat, day))\n",
        "\n",
        "    # Aggregate GROUND-side inputs (mean over replicates)\n",
        "    row = {\n",
        "        \"BaseMaterial\": mat,\n",
        "        \"DayInt\": float(day),\n",
        "        \"ground_coverage_mean\": float(g_block[TARGET].mean()),\n",
        "        \"ground_max_thickness_mean\": float(g_block[COL_MAX_THICK].mean()),\n",
        "        \"ground_roughness_bio_mean\": float(g_block[COL_ROUGH_BIO].mean()),\n",
        "        # Material properties (should be constant per material, but we average just in case):\n",
        "        \"Material_Roughness\": float(g_block[COL_MAT_ROUGH].mean()),\n",
        "        \"Contact_Angle\": float(g_block[COL_CA].mean()),\n",
        "        \"Cos_theta\": float(g_block[COL_COSTH].mean()),\n",
        "        \"Wadh\": float(g_block[COL_WADH].mean()),\n",
        "        # FLIGHT coverage = target y (mean)\n",
        "        \"flight_coverage_mean\": float(f_block[TARGET].mean()),\n",
        "    }\n",
        "\n",
        "    paired_rows.append(row)\n",
        "\n",
        "paired_df = pd.DataFrame(paired_rows)\n",
        "\n",
        "if paired_df.empty:\n",
        "    raise RuntimeError(\"No (BaseMaterial, DayInt) pairs with both ground and flight data found.\")\n",
        "\n",
        "\n",
        "num_feature_cols = [\n",
        "    \"DayInt\",\n",
        "    \"ground_coverage_mean\",\n",
        "    \"ground_max_thickness_mean\",\n",
        "    \"ground_roughness_bio_mean\",\n",
        "    \"Material_Roughness\",\n",
        "    \"Contact_Angle\",\n",
        "    \"Cos_theta\",\n",
        "    \"Wadh\",\n",
        "]\n",
        "cat_feature_cols = [\"BaseMaterial\"]\n",
        "\n",
        "for c in num_feature_cols:\n",
        "    paired_df[c] = pd.to_numeric(paired_df[c], errors=\"coerce\")\n",
        "\n",
        "paired_df = paired_df.dropna(subset=num_feature_cols + [\"flight_coverage_mean\"]).copy()\n",
        "\n",
        "X = paired_df[num_feature_cols + cat_feature_cols]\n",
        "y = paired_df[\"flight_coverage_mean\"].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Preprocessing: scale numerics, one-hot encode BaseMaterial\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(with_mean=False), num_feature_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_feature_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=600,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"rf\", rf),\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# post-hoc linear calibration to fix scaling issue\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# reshape for sklearn\n",
        "X_cal = y_pred.reshape(-1, 1)\n",
        "y_cal = y_test\n",
        "\n",
        "cal = LinearRegression().fit(X_cal, y_cal)\n",
        "a = cal.intercept_\n",
        "b = cal.coef_[0]\n",
        "\n",
        "print(\"\\nCalibration fitted: y_true ≈ a + b * y_pred\")\n",
        "print(\"a =\", a, \"   b =\", b)\n",
        "\n",
        "# apply calibration\n",
        "y_pred_cal = a + b * y_pred\n",
        "\n",
        "# physical clipping to avoid negative or >100 values\n",
        "y_pred_cal = np.clip(y_pred_cal, 0.0, 100.0)\n",
        "\n",
        "\n",
        "metrics = {}\n",
        "metrics[\"strict_ground_to_flight\"] = met(y_test, y_pred_cal)\n",
        "metrics[\"strict_ground_to_flight\"].update({\n",
        "    \"calibration_a\": float(a),\n",
        "    \"calibration_b\": float(b),\n",
        "})\n",
        "metrics[\"strict_ground_to_flight\"].update({\n",
        "    \"n_pairs_total\": int(len(paired_df)),\n",
        "    \"n_train_pairs\": int(len(X_train)),\n",
        "    \"n_test_pairs\": int(len(X_test)),\n",
        "    \"numeric_features\": num_feature_cols,\n",
        "    \"categorical_features\": cat_feature_cols,\n",
        "})\n",
        "\n",
        "with open(OUTDIR / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# Save predictions (test set only)\n",
        "pred_df = X_test.copy()\n",
        "pred_df[\"y_true_flight\"] = y_test\n",
        "pred_df[\"y_pred_flight_calibrated\"] = y_pred_cal\n",
        "pred_df[\"y_pred_flight_raw\"] = y_pred  # (optional, for debugging)\n",
        "\n",
        "pred_df.to_csv(OUTDIR / \"strict_ground_to_flight_predictions.csv\", index=False)\n",
        "\n",
        "# Pred vs actual scatter\n",
        "plt.figure()\n",
        "plt.scatter(y_test, y_pred_cal)\n",
        "plt.xlabel(\"Actual Flight Coverage (mean)\")\n",
        "plt.ylabel(\"Calibrated Predicted Flight Coverage\")\n",
        "plt.title(\"Strict Ground→Flight Regression (after calibration)\")\n",
        "\n",
        "lo = float(min(y_test.min(), y_pred_cal.min()))\n",
        "hi = float(max(y_test.max(), y_pred_cal.max()))\n",
        "plt.plot([lo, hi], [lo, hi], linestyle=\"--\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTDIR / \"strict_ground_to_flight_pred_vs_actual_CALIBRATED.png\", dpi=150)\n",
        "\n",
        "plt.close()\n",
        "\n",
        "print(\"Done.\")\n",
        "print(\"Pairs total:\", len(paired_df))\n",
        "print(\"Train pairs:\", len(X_train), \" Test pairs:\", len(X_test))\n",
        "print(\"Metrics written to:\", OUTDIR / \"metrics.json\")\n",
        "print(\"Predictions CSV:\", OUTDIR / \"strict_ground_to_flight_predictions.csv\")\n",
        "print(\"Pred vs Actual plot:\", OUTDIR / \"strict_ground_to_flight_pred_vs_actual.png\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
